{
  "hash": "f7598306304890d9a0bc853d1dad2237",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Supply Chain Data Analytics\nsubtitle: Analyzing and Forcasting Supermarket Sales\nauthors:\n  - name: Stan Brouwer (2671939)\n    orchid: 0009-0006-3447-0096\n    affiliation: Vrije Universiteit\n    corresponding: true\n  - name: Liz Chen (2840693)\n    affiliation: Master TSCM\n  - name: Maaike Lamberts (2854979)\n    affiliation: Supply Chain Data analysis\n  - name: Niek Schroor\n    affiliation: Group 10\n\ndate: last-modified\nbibliography: references.bib\nnumber-sections: true\n\noutput:\n  html: default\n  pdf: default\nprefer-html: true\n---\n\n\n\n## Data selection\n\nWe analyze, forecast and interpret the [Superstore sales](https://public.tableau.com/app/sample-data/sample_-_superstore.xls) provided by [Tableau](https://public.tableau.com/app/learn/sample-data) using different statistical and machine learning methods.\n\nThe dataset provided contains information about products, sales and profits of a fictitious US company. The dataset contains about 10,000 rows with 1,850 unique product names and 17 product subcategories, covering four consecutive years on a daily basis.\n\nWe describe our work in the PDF version. However, we would like to recommend reading our quarto manuscript *here* as it contains the **relevant** R code in the Article Notebook.\n\n## Data Pre-processing\n\nThe superstore data set we selected is of high quality: At first glance (which needs to be verified during the visualization), the data appears to have been recorded regularly and without interruptions. There is no sign of a sudden structural change. Since the data are consumer products, it should contain both trends and seasonality.\nNevertheless, we have included hypothetical steps to demonstrate our understanding of the data preprocessing procedure. In detail, we did:\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# Clear workspace\nrm(list = ls())\n# Function to load (and install if necessary) dependencies\ninstall_and_load <- function(packages) {\n  install.packages(setdiff(packages, rownames(installed.packages())), dependencies = TRUE)\n  invisible(lapply(packages, require, character.only = TRUE))\n}\ninstall_and_load(c(\"tidyverse\", \"readxl\", \"ggplot2\", \"lubridate\", \"stats\", \"Amelia\",\"forecast\", \"tseries\", \"plotly\", \"stringr\", \"knitr\", \"kableExtra\"))\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: tidyverse\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nLoading required package: readxl\n\nLoading required package: Amelia\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning: package 'Amelia' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: Rcpp\n## \n## Amelia II: Multiple Imputation\n## (Version 1.8.3, built: 2024-11-07)\n## Copyright (C) 2005-2024 James Honaker, Gary King and Matthew Blackwell\n## Refer to http://gking.harvard.edu/amelia/ for more information\n## \nLoading required package: forecast\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning: package 'forecast' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \nLoading required package: tseries\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning: package 'tseries' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: plotly\n\nAttaching package: 'plotly'\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nLoading required package: knitr\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning: package 'knitr' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: kableExtra\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n```\n\n\n:::\n:::\n\n\n\n-   Remove whitespaces from column names\n-   Remove the <code>Row_ID</code> column as it can be inferred by it's index\n-   Remove all columns with a single unique value, as storing these would be [redundant](https://few.vu.nl/~molenaar/courses/StatR/chapters/B-06-raw_data.html)\n-   Ensure machine-readable date formats in <code>yyyy-mm-dd</code> as these usually differ per locale.\n-   Ensure proper decimal separators\n-   Calculate the number of missing values (both NA and empty string \"\") per column.\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# Load the data\nsuppressWarnings({data <- read_excel(\"data/sample_-_superstore.xls\")}) # The Postal code column is stored as 'text' but coerced to numeric, causing warnings which we suppress\n\n# Improve column names\ncolnames(data) <- str_replace_all(colnames(data), \" \", \"_\")\ncolnames(data) <- str_replace_all(colnames(data), \"-\", \"_\")\n\n# Remove the 'Row_ID' column as it can be inferred by it's index\ndata <- subset(data, select = -`Row_ID`)\n\n# Remove all columns that have only one unique value, as storing these would be redundant\ndata <- data[, sapply(data, function(col) length(unique(col)) > 1)]\n\n# Ensure a machine-readable date format as these are usually horrible in excel files\ndata$Order_Date <- as.Date(data$Order_Date, format = \"%Y-%m-%d\")\ndata$Ship_Date <- as.Date(data$Ship_Date, format = \"%Y-%m-%d\")\n\n# The readxl package by default uses the correct decimal separator (as opposed to base R)\n\n# Calculate the number of missing values per column.\n# Origional dates and R date objects are in unix time, which return NA when compared to text (empty string). These dates are stored as 'double' datatype, Thus we check character columns for empty strings, and all columns for NA values. \nmissing_values <- sapply(data, function(col) {\n  if (inherits(col, \"Date\")) {\n    sum(is.na(col))\n  } else if (is.character(col)) {\n    sum(is.na(col) | col == \"\")\n  } else {\n    sum(is.na(col))\n  }\n})\n\n# sum(missing_values) returns 0!\n\n# Optionally, print the missing values as a nice table\nmissing_values_table <- data.frame(\n  Column = names(missing_values),\n  Missing_or_Empty = missing_values\n)\n# Note that there are no missing values, thus we do not print them\n# kable(missing_values_table, caption = \"Missing or Empty Values in Columns\", format = \"pipe\")\n\n\nrm(missing_values, missing_values_table)\n```\n:::\n\n\n\nAfter these steps (and transposing the table for better document formatting), the data looks as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nkable(t(head(data, 3)), caption = \"First 3 Rows of the Data (Transposed)\", format = \"markdown\")\n```\n\n::: {.cell-output-display}\n\n\nTable: First 3 Rows of the Data (Transposed)\n\n|              |                                  |                                                            |                                                          |\n|:-------------|:---------------------------------|:-----------------------------------------------------------|:---------------------------------------------------------|\n|Order_ID      |CA-2016-152156                    |CA-2016-152156                                              |CA-2016-138688                                            |\n|Order_Date    |2016-11-08                        |2016-11-08                                                  |2016-06-12                                                |\n|Ship_Date     |2016-11-11                        |2016-11-11                                                  |2016-06-16                                                |\n|Ship_Mode     |Second Class                      |Second Class                                                |Second Class                                              |\n|Customer_ID   |CG-12520                          |CG-12520                                                    |DV-13045                                                  |\n|Customer_Name |Claire Gute                       |Claire Gute                                                 |Darrin Van Huff                                           |\n|Segment       |Consumer                          |Consumer                                                    |Corporate                                                 |\n|City          |Henderson                         |Henderson                                                   |Los Angeles                                               |\n|State         |Kentucky                          |Kentucky                                                    |California                                                |\n|Postal_Code   |42420                             |42420                                                       |90036                                                     |\n|Region        |South                             |South                                                       |West                                                      |\n|Product_ID    |FUR-BO-10001798                   |FUR-CH-10000454                                             |OFF-LA-10000240                                           |\n|Category      |Furniture                         |Furniture                                                   |Office Supplies                                           |\n|Sub_Category  |Bookcases                         |Chairs                                                      |Labels                                                    |\n|Product_Name  |Bush Somerset Collection Bookcase |Hon Deluxe Fabric Upholstered Stacking Chairs, Rounded Back |Self-Adhesive Address Labels for Typewriters by Universal |\n|Sales         |261.96                            |731.94                                                      |14.62                                                     |\n|Quantity      |2                                 |3                                                           |2                                                         |\n|Discount      |0                                 |0                                                           |0                                                         |\n|Profit        |41.9136                           |219.5820                                                    |6.8714                                                    |\n\n\n:::\n:::\n\n\n\nWe did not find any missing values, confirming the quality of the data set. There is some more processing to do, for instance the removal of outliers. However, by doing so we impose our own assumptions on the data. Let's start by evaluating the descriptive statistics of our data and check if further processing is required.\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\ndescriptive_statistics <- function(column) {\n  if (is.numeric(column)) {\n    stats <- list(\n      Min = min(column, na.rm = TRUE), # Note that handling NA values increases robustness (and I copied the funciton from some of my earlier work)\n      Max = max(column, na.rm = TRUE),\n      Mean = mean(column, na.rm = TRUE),\n      Median = median(column, na.rm = TRUE),\n      StdDev = sd(column, na.rm = TRUE)\n    )\n  } else if (inherits(column, \"Date\")) {\n    stats <- list(\n      Earliest = format(min(column, na.rm = TRUE), \"%Y-%m-%d\"),\n      Latest = format(max(column, na.rm = TRUE), \"%Y-%m-%d\")\n    )\n  } else if (is.character(column)) {\n    stats <- list(\n      Unique = length(unique(column)),\n      Mode = names(sort(table(column), decreasing = TRUE)[1])\n    )\n  } else {\n    stats <- NULL\n  }\n  return(stats)\n}\n\n# Call function on dataframe\ndescriptive_stats <- lapply(data, descriptive_statistics)\n\n# Separate to tables dependent on data type\nnumeric_stats <- as.data.frame(do.call(rbind, lapply(names(data), function(col_name) {\n  if (is.numeric(data[[col_name]])) {\n    c(Column = col_name, descriptive_stats[[col_name]])\n  }\n})), stringsAsFactors = FALSE)\ndate_stats <- as.data.frame(do.call(rbind, lapply(names(data), function(col_name) {\n  if (inherits(data[[col_name]], \"Date\")) {\n    c(Column = col_name, descriptive_stats[[col_name]])\n  }\n})), stringsAsFactors = FALSE)\ncharacter_stats <- as.data.frame(do.call(rbind, lapply(names(data), function(col_name) {\n  if (is.character(data[[col_name]])) {\n    c(Column = col_name, descriptive_stats[[col_name]])\n  }\n})), stringsAsFactors = FALSE)\n```\n:::\n\n::: {.cell Label='Descriptive_Output_tables'}\n\n```{.r .cell-code .hidden}\nkable(\n  numeric_stats,\n  caption = \"Descriptive Statistics for Numeric Columns\",\n  format = \"pipe\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Descriptive Statistics for Numeric Columns\n\n|Column      |Min       |Max      |Mean      |Median  |StdDev   |\n|:-----------|:---------|:--------|:---------|:-------|:--------|\n|Postal_Code |1040      |99301    |55190.38  |56430.5 |32063.69 |\n|Sales       |0.444     |22638.48 |229.858   |54.49   |623.2451 |\n|Quantity    |1         |14       |3.789574  |3       |2.22511  |\n|Discount    |0         |0.8      |0.1562027 |0.2     |0.206452 |\n|Profit      |-6599.978 |8399.976 |28.6569   |8.6665  |234.2601 |\n\n\n:::\n\n```{.r .cell-code .hidden}\nkable(\n  date_stats,\n  caption = \"Descriptive Statistics for Date Columns\",\n  format = \"pipe\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Descriptive Statistics for Date Columns\n\n|Column     |Earliest   |Latest     |\n|:----------|:----------|:----------|\n|Order_Date |2014-01-03 |2017-12-30 |\n|Ship_Date  |2014-01-07 |2018-01-05 |\n\n\n:::\n:::\n\n\nWe inspect the orders with the lowest and highest Sales amount (in USD). The most expensive orders were professional printers, cameras and teleconferencing units with high unit prices. The orders with the lowest sales amount were often binders and had a high Discount rate.\n\nInterestingly there are orders with a negative profit. They typically have high Discount rates and often concern the same item, such as the “Cubify CubeX 3D Printer Triple Head Print”. The orders with a negative Profit were often part of a larger order (for instance CA-2016-108196), and placed by customers with multiple orders. We suspect these negative Profit's to be caused by items of lower quality that receive discounts, general discount codes, or volume discounts. However, due to the high discounts especially on orders with negative profit, we assume these to be valid orders. \n\n** Some negative profit products **\n\nIn figure x we plotted the quantities of the most sold products. Unfortunately, the sold quantities of individual products were too low to determine any meaningful trends.\n\n\n\n::: {.cell warnng='false'}\n\n```{.r .cell-code .hidden}\n# Optionally: print top 10 sale quantity barplot\n# # Sum of Quantity for top products\n# top_products <- data %>%\n#   group_by(Product_Name) %>%\n#   summarize(total_quantity = sum(Quantity, na.rm = TRUE)) %>%\n#   arrange(desc(total_quantity)) %>%\n#   slice_head(n = 10) %>% \n#   mutate(ProdName8 = substr(Product_Name, 1, 8)) # Truncate product names to the first 8 characters. Long names mess up formatting\n# \n# # Plot\n# ggplot(top_products, aes(x = reorder(ProdName8, -total_quantity), y = total_quantity)) +\n#   geom_bar(stat = \"identity\", fill = \"steelblue\") +\n#   labs(title = \"Top 20 Most Sold Products\",\n#        x = \"Product ID\",\n#        y = \"Total Quantity\") +\n#   theme_minimal() +\n#   coord_flip()\n\n# Aggregate quantity by Product Name and Order Date to create a time series\ntime_series_data <- data %>%\n  group_by(Product_Name, Order_Date) %>%\n  summarize(total_quantity = sum(Quantity, na.rm = TRUE)) %>%\n  ungroup()\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n`summarise()` has grouped output by 'Product_Name'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# Filter for the top products by total quantity sold (adjust as needed)\ntop_products <- time_series_data %>%\n  group_by(Product_Name) %>%\n  summarize(total_quantity = sum(total_quantity)) %>%\n  arrange(desc(total_quantity)) %>%\n  slice_head(n = 10)  # Select top 10 products\n\n# Filter the time-series data for only these top products\nfiltered_time_series_data <- time_series_data %>%\n  filter(Product_Name %in% top_products$Product_Name) %>%\n  mutate(ProdName10 = substr(Product_Name, 1, 10)) # Product names can be quite long and mess up layouts. Lets only plot the first 10 chars.\n\n# Here we do some special plotting. We want to show the plot with only one selected line by default, but make sure that the other 9 top sold products can be selected. We first create the ggplotly object, and than modify the visibility of the traces\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# Plot interactive figure in html, plot ggplot2 in pdf:\n\n# Creating the ggplotly object\np_ly <- ggplotly(ggplot(filtered_time_series_data, aes(x = Order_Date, y = total_quantity, color = ProdName10)) +\n  geom_line(size = 1) +\n  labs(title = \"Quantity Sold Over Time per Product\",\n       x = \"Order Date\",\n       y = \"Quantity Sold\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_color_discrete(name = \"Product Name\"))\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# Modify the visibility of traces\nfor (i in seq_along(p_ly$x$data)) {\n  if (i == 1) {\n    p_ly$x$data[[i]]$visible <- TRUE  # Make the first trace visible\n  } else {\n    p_ly$x$data[[i]]$visible <- \"legendonly\"  # Hide the rest\n  }\n}\n\n# Plot\np_ly\n```\n\n::: {.cell-output-display}\n![Figure X Sale quantity of the most popular products](index_files/figure-docx/Top_Products_Quantity-1.png)\n:::\n:::\n\n\n\nOur proposed workaround is to aggregate <code>Product_Name</code> by <code>Sub_Category</code>, and treat it as a single product for the rest of the assignment, which we plotted in figure X.\n\n\n\n::: {.cell fig-caption='Aggregated Sub_Category sales (toggle )'}\n\n```{.r .cell-code .hidden}\n# Bar plots\n\n# # Count frequency of top 20 products\n# top_products <- data %>%\n#   count(Product_Name, sort = TRUE) %>%\n#   top_n(20, n) %>%\n#   mutate(ProdName8 = substr(Product_Name, 1, 8))\n# \n# # Plot!\n# ggplot(top_products, aes(x = reorder(`ProdName8`, -n), y = n)) +\n#   geom_bar(stat = \"identity\", fill = \"steelblue\") +\n#   labs(title = \"Top 20 Most Sold Products\",\n#        x = \"Product Name\",\n#        y = \"Quantity sold\") +\n#   theme_minimal() +\n#   coord_flip()\n# \n# Count frequency of top 20 products\ntop_categories <- data %>%\n  count(Sub_Category, sort = TRUE)\n# \n# # Plot!\n# ggplot(top_categories, aes(x = reorder(Sub_Category, -n), y = n)) +\n#   geom_bar(stat = \"identity\", fill = \"steelblue\") +\n#   labs(title = \"Sub_Categories sorted\",\n#        x = \"Product Name\",\n#        y = \"Quantity sold\") +\n#   theme_minimal() +\n#   coord_flip()\n\n# Find top 10 most sold product names\ntop_10_categories <- top_categories$Sub_Category[0:10]\n\n# Filter the data for  top 10 products\ntop_10_data <- data %>% filter(Sub_Category %in% top_10_categories)\n\n# calculate sales per month\ntop_10_data <- top_10_data %>%\n  mutate(Month = floor_date(Order_Date, unit = \"month\"))\n\n# Aggregate data by month for each sub-category\ntop_10_data_aggregated <- top_10_data %>%\n  group_by(Month, Sub_Category) %>%\n  summarise(Sales_Count = n(), .groups = 'drop')\n\n# Some special interactive plot formatting (see previous plot)\np_ly <- ggplotly(ggplot(top_10_data_aggregated, aes(x = Month, y = Sales_Count, color = Sub_Category, group = Sub_Category)) +\n    geom_line(size = 1) +\n    geom_point(size = 2) +\n    labs(title = \"Monthly Sales for the Top 3 Most Sold Sub Categories\",\n         x = \"Month\",\n         y = \"Sales Count\",\n         color = \"Sub Category\") +\n    theme_minimal())\n\n# Modify the visibility of traces\nfor (i in seq_along(p_ly$x$data)) {\n  if (i == 1) {\n    p_ly$x$data[[i]]$visible <- TRUE  # Make the first trace visible\n  } else {\n    p_ly$x$data[[i]]$visible <- \"legendonly\"  # Hide the rest\n  }\n}\n\n# Plot\np_ly\n```\n\n::: {.cell-output-display}\n![](index_files/figure-docx/Aggregated_Sub_Category_sales-1.png)\n:::\n:::\n\n\n\nThis aggregated Quantity starts to show trends and seasonality, and is much more useful to base predictions on! We will use these aggregated sub-categories for the rest of the assignment.\n\nTo properly finish our data preprocessing we ran some statistics on Quantity aggregated by Sub_Category. Table x contains some descriptive statistics.\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(dplyr)\nlibrary(kableExtra)\n\n# Summarize the data\noutlier_summary <- data %>%\n  group_by(Sub_Category) %>%\n  summarize(\n    Min = round(min(Quantity), 2),\n    Mean = round(mean(Quantity), 2),\n    Max = round(max(Quantity), 2),\n    Sd = round(sd(Quantity), 2),\n    CI_lower = round(Mean - 1.96 * (Sd / sqrt(n())), 2),\n    CI_upper = round(Mean + 1.96 * (Sd / sqrt(n())), 2),\n    .groups = \"drop\"\n  )\n\n# Output tables\nkable(\n  outlier_summary,\n  caption = \"Statistics for Sub_Category quantity\",\n  format = \"pipe\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Statistics for Sub_Category quantity\n\n|Sub_Category | Min| Mean| Max|   Sd| CI_lower| CI_upper|\n|:------------|---:|----:|---:|----:|--------:|--------:|\n|Accessories  |   1| 3.84|  14| 2.28|     3.68|     4.00|\n|Appliances   |   1| 3.71|  14| 2.12|     3.52|     3.90|\n|Art          |   1| 3.77|  14| 2.13|     3.62|     3.92|\n|Binders      |   1| 3.92|  14| 2.29|     3.80|     4.04|\n|Bookcases    |   1| 3.81|  13| 2.28|     3.51|     4.11|\n|Chairs       |   1| 3.82|  14| 2.28|     3.64|     4.00|\n|Copiers      |   1| 3.44|   9| 1.83|     3.01|     3.87|\n|Envelopes    |   1| 3.57|   9| 2.05|     3.32|     3.82|\n|Fasteners    |   1| 4.21|  14| 2.41|     3.89|     4.53|\n|Furnishings  |   1| 3.72|  14| 2.16|     3.58|     3.86|\n|Labels       |   1| 3.85|  14| 2.35|     3.61|     4.09|\n|Machines     |   1| 3.83|  11| 2.17|     3.43|     4.23|\n|Paper        |   1| 3.78|  14| 2.23|     3.66|     3.90|\n|Phones       |   1| 3.70|  14| 2.19|     3.56|     3.84|\n|Storage      |   1| 3.73|  14| 2.19|     3.58|     3.88|\n|Supplies     |   1| 3.41|  10| 1.84|     3.15|     3.67|\n|Tables       |   1| 3.89|  13| 2.45|     3.62|     4.16|\n\n\n:::\n:::\n\n\n\nThe statistics for <code>Quantity</code> aggregated by <code>Sub_Category</code> looks valid. We can visualize it as histogram and check for anomalies. Figure y contains histograms of <code>Quantity</code> per <code>Sub_Category</code>.\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nsub_categories <- unique(data$Sub_Category)\n\np <- plot_ly()\nfor (i in seq_along(sub_categories)) {\n  sub <- sub_categories[i]\n  subset_data <- data %>% filter(Sub_Category == sub)\n  p <- add_trace(\n    p,\n    x = subset_data$Quantity,\n    type = \"histogram\",\n    name = sub,\n    visible = ifelse(i == 1, TRUE, FALSE)\n  )\n}\n\n# We add a drop down menu for Sub_Category as toggling visibility in default ggplot2 adds the histograms up. Instead we want to be able to show each histogram seperately. \ndropdown_buttons <- lapply(seq_along(sub_categories), function(i) {\n  list(\n    method = \"update\",\n    args = list(\n      list(visible = lapply(seq_along(sub_categories), function(j) j == i)),\n      list(xaxis = list(title = \"Quantity\", autorange = TRUE), \n           yaxis = list(title = \"Frequency\", autorange = TRUE))\n    ),\n    label = sub_categories[i]\n  )\n})\n\n# Style drop down layout\np <- p %>%\n  layout(\n    title = \"Distribution of Quantity Sold per Order by Sub Category\",\n    xaxis = list(title = \"Quantity\"),\n    yaxis = list(title = \"Frequency\"),\n    showlegend = FALSE,  # Drop down instead of legend\n    updatemenus = list(\n      list(\n        type = \"dropdown\",\n        buttons = dropdown_buttons,\n        direction = \"down\",\n        x = 0.99,\n        y = 0.99,\n        showactive = TRUE,\n        xanchor = \"left\",\n        yanchor = \"top\"\n      )\n    )\n  )\np\n```\n\n::: {.cell-output-display}\n![](index_files/figure-docx/sub_category_histograms-1.png)\n:::\n:::\n\n\n\nThe histograms show that the quantities a right-skewed distributed. This is to be expected since most orders contain only a small number of items. We will not remove the outliers with large quantities since they appear valid..\n\n## Data Visualization\n\n<hr>\n\n## Forecasting Method Evaluation\n\n### Forecasting top 3 product categories (4a)\n\nLet's forecast sold quantities for the three most sold sub-categories:\n\nThe steps taken for data preparation were:\n\n-   Identifying Top Subcategories: The top three subcategories are selected from our dataset based on their sold quantities. The top three were: Binders, furnishing and paper.\n-   The sold quantities are aggregated monthly to create a time series object which we can use in the forecasting.\n-   A KPSS showed that the data is non stationary. First-order differencing is applied to transform the data from non-stationary to stationary. The KPSS results in a p-value >0.05 showing the stationarity. \n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# Find top 3 most sold product names\ntop_categories <- data %>%\n  group_by(Sub_Category) %>%\n  summarise(Total_Quantity = sum(Quantity)) %>%\n  arrange(desc(Total_Quantity))\ntop_3_subcategories <- top_categories$Sub_Category[0:3]\n\n# Filter the data for  top 3 products\ntop_3_data <- data %>% filter(Sub_Category %in% top_3_subcategories)\n\n# calculate sales per month\ntop_3_data <- top_3_data %>%\n  mutate(Month = floor_date(Order_Date, unit = \"month\"))\n\n# Aggregate data by month for each product\ntop_3_data_aggregated <- top_3_data %>%\n  group_by(Month, Sub_Category) %>%\n  summarise(Sales_Count = n(), .groups = 'drop')\n\n# Create a time series object for each product\nts_data <- top_3_data_aggregated %>%\n  pivot_wider(names_from = Sub_Category, values_from = Sales_Count, values_fill = 0) %>%\n  select(-Month) %>%\n  as.matrix()\n\n# Create a time series object\nts_data <- ts(ts_data, start = c(2014, 1), end = c(2017, 12), frequency = 12)\n\n# Create a time series list for each subcategory\nts_list <- list()\n\nfor (subcategory in top_3_subcategories) {\n  # Filter data for the subcategory\n  subcategory_data <- top_3_data_aggregated %>% filter(Sub_Category == subcategory)\n\n  # Create a time series object (assuming monthly data from January 2014 to December 2017)\n  ts_list[[subcategory]] <- ts(subcategory_data$Sales_Count,\n                                start = c(2014, 1),\n                                end = c(2017, 12),\n                                frequency = 12)\n}\n\n#### 4 A\n# Step 4: Apply forecasting methods to the top 3 sub-categories\nforecast_results <- list()  # Store results\n\nfor (subcategory in names(ts_list)) {\n  ts_current <- ts_list[[subcategory]]\n\n  # Split the data into training and validation sets (70% training, 30% testing)\n  train_size <- floor(0.7 * length(ts_current))\n  train_ts <- window(ts_current, end = c(2014 + (train_size - 1) %/% 12, (train_size - 1) %% 12 + 1))\n  test_ts <- window(ts_current, start = c(2014 + train_size %/% 12, train_size %% 12 + 1))\n\n  # 1. ARIMA\n  arima_model <- auto.arima(train_ts)\n  arima_forecast <- forecast(arima_model, h = length(test_ts))\n  arima_accuracy <- accuracy(arima_forecast, test_ts)\n\n  # 2. Holt-Winters\n  hw_model <- HoltWinters(train_ts)\n  hw_forecast <- forecast(hw_model, h = length(test_ts))\n  hw_accuracy <- accuracy(hw_forecast, test_ts)\n\n  # 3. ETS\n  ets_model <- ets(train_ts)\n  ets_forecast <- forecast(ets_model, h = length(test_ts))\n  ets_accuracy <- accuracy(ets_forecast, test_ts)\n\n  # Store results\n  forecast_results[[subcategory]] <- list(\n    ARIMA = list(Model = arima_model, Forecast = arima_forecast, Accuracy = arima_accuracy),\n    HoltWinters = list(Model = hw_model, Forecast = hw_forecast, Accuracy = hw_accuracy),\n    ETS = list(Model = ets_model, Forecast = ets_forecast, Accuracy = ets_accuracy)\n  )\n}\n\n\n# For formatting, we ommitted almost all output. You can uncomment the code and check your output if you like.\n\n# # Step 5: Print results\n#  for (subcategory in names(forecast_results)) {\n#   cat(\"\\n\\nResults for Sub_Category:\", subcategory, \"\\n\")\n# \n#   cat(\"\\nARIMA Accuracy:\\n\")\n#   print(forecast_results[[subcategory]]$ARIMA$Accuracy)\n# \n#   cat(\"\\nHolt-Winters Accuracy:\\n\")\n#   print(forecast_results[[subcategory]]$HoltWinters$Accuracy)\n# \n#   cat(\"\\nETS Accuracy:\\n\")\n#   print(forecast_results[[subcategory]]$ETS$Accuracy)\n# }\n```\n:::\n\n\n\nThree models are applied to each subcategory to forecast it. The models we use are: ARIMA, Holt-Winters and ETS. We have chosen these models because of their level of suitability for discrete time series data with all different levels of trend and seasonality. To evaluate the methods and its effectiveness , the data is split into a training set (70%) and testing set (30%). \n\nTo assess the results, we use the following performance metrics: ME, RMSE, MAE and MAPE. They are calculated for the training and testing phases of the forecast. \n\nAs we can see on the forecasting results ARIMA performed well for binders. We can state this because of the lowest RMSE if you compare it to the other models.\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# Lets plot the results\nfor (subcategory in names(forecast_results)) {\n  ts_current <- ts_list[[subcategory]]\n  train_size <- floor(0.7 * length(ts_current))\n  test_ts <- window(ts_current, start = c(2014 + train_size %/% 12, train_size %% 12 + 1))\n  arima_forecast <- forecast_results[[subcategory]]$ARIMA$Forecast\n  hw_forecast <- forecast_results[[subcategory]]$HoltWinters$Forecast\n  ets_forecast <- forecast_results[[subcategory]]$ETS$Forecast\n  \n  # Combined plot\n  plot(arima_forecast$mean, col = \"blue\", lwd = 2, \n       ylim = range(c(arima_forecast$mean, hw_forecast$mean, ets_forecast$mean, test_ts)),\n       main = paste(\"Combined Forecasts for\", subcategory, \"before differencing\"),\n       xlab = \"Time\", ylab = \"Forecasted Values\")\n  lines(test_ts, col = \"red\", lty = 2, lwd = 2)\n  lines(hw_forecast$mean, col = \"green\", lwd = 2)\n  lines(ets_forecast$mean, col = \"purple\", lwd = 2)\n  legend(\"topleft\", legend = c(\"ARIMA\", \"Holt-Winters\", \"ETS\", \"Test Data\"),\n         col = c(\"blue\", \"green\", \"purple\", \"red\"), lty = c(1, 1, 1, 2), lwd = 2)\n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-docx/Results1-1.png)\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-docx/Results1-2.png)\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-docx/Results1-3.png)\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\noptions(warn = -1)\n# # Step 6: Visualization of Forecasts\n# for (subcategory in names(forecast_results)) {\n#   plot(forecast_results[[subcategory]]$ARIMA$Forecast, main = paste(\"ARIMA Forecast for\", subcategory))\n#   lines(test_ts, col = \"red\", lty = 2)\n# \n#   plot(forecast_results[[subcategory]]$HoltWinters$Forecast, main = paste(\"Holt-Winters Forecast for\", subcategory))\n#   lines(test_ts, col = \"red\", lty = 2)\n# \n#   plot(forecast_results[[subcategory]]$ETS$Forecast, main = paste(\"ETS Forecast for\", subcategory))\n#   lines(test_ts, col = \"red\", lty = 2)\n# }\n\n#more stationary tests\n# Perform KPSS Test for the top 3 subcategories\n# top_3_subcategories <- top_categories$Sub_Category[0:3]\n# \n# for (subcategory in top_3_subcategories) {\n#   if (subcategory %in% names(ts_list)) {\n#     ts_current <- ts_list[[subcategory]]\n#     cat(\"\\nKPSS Test for Sub-Category:\", subcategory, \"\\n\")\n#     print(kpss.test(ts_current))\n#   } else {\n#     cat(\"\\nSub-Category not found in ts_list:\", subcategory, \"\\n\")\n#   }\n# }\n\n#Because the all the 3 subcategory are non stationary because of a P value which is <=0.05 we need to use differencing\n# Apply differencing to each of the top 3 subcategories\ndifferenced_series <- list()\n\nfor (subcategory in top_3_subcategories) {\n  if (subcategory %in% names(ts_list)) {\n    ts_current <- ts_list[[subcategory]]  # Get the time series for the subcategory\n    ts_diff <- diff(ts_current, differences = 1)  # Apply first-order differencing\n    differenced_series[[subcategory]] <- ts_diff  # Store the differenced series\n\n    # Recheck stationarity with KPSS test\n    #cat(\"\\nKPSS Test for Differenced Sub-Category:\", subcategory, \"\\n\")\n    print(kpss.test(ts_diff))\n  } else {\n    #cat(\"\\nSub-Category not found in ts_list:\", subcategory, \"\\n\")\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tKPSS Test for Level Stationarity\n\ndata:  ts_diff\nKPSS Level = 0.10182, Truncation lag parameter = 3, p-value = 0.1\n\n\n\tKPSS Test for Level Stationarity\n\ndata:  ts_diff\nKPSS Level = 0.061982, Truncation lag parameter = 3, p-value = 0.1\n\n\n\tKPSS Test for Level Stationarity\n\ndata:  ts_diff\nKPSS Level = 0.098438, Truncation lag parameter = 3, p-value = 0.1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# Time differenced plots\n# # Now P value is larger then 0.05 so we have stationary data\n# # Plot the differenced series for each subcategory\n# for (subcategory in top_3_subcategories) {\n#   if (subcategory %in% names(differenced_series)) {\n#     ts_diff <- differenced_series[[subcategory]]\n#     cat(\"\\nPlotting Differenced Series for Sub-Category:\", subcategory, \"\\n\")\n#     plot(ts_diff, main = paste(\"Differenced Series for Sub-Category:\", subcategory),\n#          ylab = \"Differenced Values\", xlab = \"Time\")\n#   }\n# }\n\n\n# Combine differenced series plots for all top subcategories\ncombined_diff_plot <- function(differenced_series, top_3_subcategories) {\n  plot(NULL, xlim = range(time(differenced_series[[top_3_subcategories[1]]])), \n       ylim = range(sapply(differenced_series[top_3_subcategories], range, na.rm = TRUE)),\n       xlab = \"Time\", ylab = \"Differenced Values\",\n       main = \"Differenced Series for Top Subcategories\")\n  colors <- c(\"blue\", \"green\", \"purple\")\n  for (i in seq_along(top_3_subcategories)) {\n    subcategory <- top_3_subcategories[i]\n    if (subcategory %in% names(differenced_series)) {\n      ts_diff <- differenced_series[[subcategory]]\n      lines(ts_diff, col = colors[i], lwd = 2, lty = i)\n    }\n  }\n  legend(\"topright\", legend = top_3_subcategories, \n         col = colors, lty = 1:length(top_3_subcategories), lwd = 2)\n}\ncombined_diff_plot(differenced_series, top_3_subcategories)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-docx/results3-1.png)\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n#NEW FORECASTING FOR 4A with stationary data\n# Step 4: Apply forecasting methods to the differenced top 3 sub-categories\nforecast_results <- list()  # Store results\n\nfor (subcategory in names(differenced_series)) {\n  ts_current <- differenced_series[[subcategory]]  # Use the differenced series\n\n  # Split the data into training and validation sets (70% training, 30% testing)\n  train_size <- floor(0.7 * length(ts_current))\n  train_ts <- window(ts_current, end = c(2014 + (train_size - 1) %/% 12, (train_size - 1) %% 12 + 1))\n  test_ts <- window(ts_current, start = c(2014 + train_size %/% 12, train_size %% 12 + 1))\n\n  # 1. ARIMA\n  arima_model <- auto.arima(train_ts)\n  arima_forecast <- forecast(arima_model, h = length(test_ts))\n  arima_accuracy <- accuracy(arima_forecast, test_ts)\n\n  # 2. Holt-Winters\n  hw_model <- HoltWinters(train_ts)\n  hw_forecast <- forecast(hw_model, h = length(test_ts))\n  hw_accuracy <- accuracy(hw_forecast, test_ts)\n\n  # 3. ETS\n  ets_model <- ets(train_ts)\n  ets_forecast <- forecast(ets_model, h = length(test_ts))\n  ets_accuracy <- accuracy(ets_forecast, test_ts)\n\n  # Store results\n  forecast_results[[subcategory]] <- list(\n    ARIMA = list(Model = arima_model, Forecast = arima_forecast, Accuracy = arima_accuracy),\n    HoltWinters = list(Model = hw_model, Forecast = hw_forecast, Accuracy = hw_accuracy),\n    ETS = list(Model = ets_model, Forecast = ets_forecast, Accuracy = ets_accuracy)\n  )\n}\n\n# # Step 5: Print results\n# for (subcategory in names(forecast_results)) {\n#   cat(\"\\n\\nResults for Sub_Category:\", subcategory, \"\\n\")\n# \n#   cat(\"\\nARIMA Accuracy:\\n\")\n#   print(forecast_results[[subcategory]]$ARIMA$Accuracy)\n# \n#   cat(\"\\nHolt-Winters Accuracy:\\n\")\n#   print(forecast_results[[subcategory]]$HoltWinters$Accuracy)\n# \n#   cat(\"\\nETS Accuracy:\\n\")\n#   print(forecast_results[[subcategory]]$ETS$Accuracy)\n# }\n\n# # Step 6: Visualization of Forecasts\n# for (subcategory in names(forecast_results)) {\n#   plot(forecast_results[[subcategory]]$ARIMA$Forecast,\n#        main = paste(\"ARIMA Forecast for\", subcategory),\n#        ylab = \"Differenced Values\", xlab = \"Time\")\n#   lines(test_ts, col = \"red\", lty = 2)\n# \n#   plot(forecast_results[[subcategory]]$HoltWinters$Forecast,\n#        main = paste(\"Holt-Winters Forecast for\", subcategory),\n#        ylab = \"Differenced Values\", xlab = \"Time\")\n#   lines(test_ts, col = \"red\", lty = 2)\n# \n#   plot(forecast_results[[subcategory]]$ETS$Forecast,\n#        main = paste(\"ETS Forecast for\", subcategory),\n#        ylab = \"Differenced Values\", xlab = \"Time\")\n#   lines(test_ts, col = \"red\", lty = 2)\n# }\n\n# Lets plot the results\nfor (subcategory in names(forecast_results)) {\n  ts_current <- ts_list[[subcategory]]\n  train_size <- floor(0.7 * length(ts_current))\n  test_ts <- window(ts_current, start = c(2014 + train_size %/% 12, train_size %% 12 + 1))\n  arima_forecast <- forecast_results[[subcategory]]$ARIMA$Forecast\n  hw_forecast <- forecast_results[[subcategory]]$HoltWinters$Forecast\n  ets_forecast <- forecast_results[[subcategory]]$ETS$Forecast\n  \n  # Combined plot\n  plot(arima_forecast$mean, col = \"blue\", lwd = 2, \n       ylim = range(c(arima_forecast$mean, hw_forecast$mean, ets_forecast$mean, test_ts)),\n       main = paste(\"Combined Forecasts for\", subcategory, \"before differencing\"),\n       xlab = \"Time\", ylab = \"Forecasted Values\")\n  lines(test_ts, col = \"red\", lty = 2, lwd = 2)\n  lines(hw_forecast$mean, col = \"green\", lwd = 2)\n  lines(ets_forecast$mean, col = \"purple\", lwd = 2)\n  legend(\"topleft\", legend = c(\"ARIMA\", \"Holt-Winters\", \"ETS\", \"Test Data\"),\n         col = c(\"blue\", \"green\", \"purple\", \"red\"), lty = c(1, 1, 1, 2), lwd = 2)\n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-docx/Results4-1.png)\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-docx/Results4-2.png)\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-docx/Results4-3.png)\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# final KPSS test\n\n# Perform KPSS Test for the differenced series in the top 3 subcategories\nfor (subcategory in top_3_subcategories) {\n  if (subcategory %in% names(differenced_series)) {\n    ts_current <- differenced_series[[subcategory]]  # Get the differenced series\n    cat(\"\\nKPSS Test for Differenced Sub-Category:\", subcategory, \"\\n\")\n    print(kpss.test(ts_current))\n  } else {\n    cat(\"\\nSub-Category not found in differenced_series:\", subcategory, \"\\n\")\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nKPSS Test for Differenced Sub-Category: Binders \n\n\tKPSS Test for Level Stationarity\n\ndata:  ts_current\nKPSS Level = 0.10182, Truncation lag parameter = 3, p-value = 0.1\n\n\nKPSS Test for Differenced Sub-Category: Paper \n\n\tKPSS Test for Level Stationarity\n\ndata:  ts_current\nKPSS Level = 0.061982, Truncation lag parameter = 3, p-value = 0.1\n\n\nKPSS Test for Differenced Sub-Category: Furnishings \n\n\tKPSS Test for Level Stationarity\n\ndata:  ts_current\nKPSS Level = 0.098438, Truncation lag parameter = 3, p-value = 0.1\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# now they are all 0.1\n```\n:::\n\n\n\n\n\n\nAs we can see on the forecasting results ARIMA performed well for binders. We can state this because of the lowest RMSE. \n-\tARIMA Binders:\n-\tForecasting results Binders: \n\nFor the subcategory furnishings we can see that the ETS forecasting method is the most stable across the training and testing phase. \n-\tETS furnishings\n-\tForecasting results Furnishings:\n\nFor the last subcategory and product paper the ETS model is again the most consistent, comparing the statistics for training and test set. The high variability in the test data leads to larger forecasting errors in all the 3 models.\n\n-\tETS Furnishings:\n-\tForecasting results: \n\nResidual Diagnostics:\n-\tThe checks show no real autocorrelation for ARIMA models. Which indicates a good fitting forecast. \n\n### Conclusion (4a)\n\nThe most effective model is not the same in all the subcategories. Each model was validated based on its ability to capture seasonality and trend. ARIMA performed better for Binders, while ETS performed better for Furnishings and Paper. \n\n### Clustering (4b)\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# 4B\n# 4B: Group Products into Clusters and Apply Forecasting Techniques\n# Step 1: Extract Time-Series Features for Clustering\ntime_series_features <- data.frame()\n\nfor (subcategory in names(ts_list)) {\n  ts_current <- ts_list[[subcategory]]\n\n  # Decompose the time series to extract features\n  decomposition <- decompose(ts_current)\n  trend_strength <- var(decomposition$trend, na.rm = TRUE) / var(ts_current, na.rm = TRUE)\n  seasonal_strength <- var(decomposition$seasonal, na.rm = TRUE) / var(ts_current, na.rm = TRUE)\n  random_strength <- var(decomposition$random, na.rm = TRUE) / var(ts_current, na.rm = TRUE)\n\n  # Store extracted features\n  time_series_features <- rbind(time_series_features,\n                                data.frame(SubCategory = subcategory,\n                                           TrendStrength = trend_strength,\n                                           SeasonalStrength = seasonal_strength,\n                                           RandomStrength = random_strength))\n}\n\n# Step 2: Normalize the Features for Clustering\ntime_series_features_scaled <- time_series_features %>%\n  select(-SubCategory) %>%\n  scale()\n\n# verify rows\n# nrow(time_series_features_scaled)\n\n# Step 3: Perform K-Means Clustering\n# Determine the number of clusters dynamically\nk <- min(3, nrow(time_series_features_scaled))  # Set k to the smaller of 3 or the number of rows\n# Hierarchical Clustering\ndistance_matrix <- dist(time_series_features_scaled)  # Calculate distance matrix\nhc <- hclust(distance_matrix)  # Perform hierarchical clustering\ntime_series_features$Cluster <- cutree(hc, k = k)  # Cut tree into 'k' clusters\n# Add cluster information to the original data\ntime_series_features$Cluster <- cutree(hc, k = k)\n\n# Step 4: Apply Forecasting Techniques to Each Cluster\nforecast_results_by_cluster <- list()\n\nfor (cluster_id in unique(time_series_features$Cluster)) {\n  # Get subcategories in the current cluster\n  subcategories_in_cluster <- time_series_features$SubCategory[time_series_features$Cluster == cluster_id]\n\n  # Initialize storage for cluster results\n  cluster_forecast_results <- list()\n\n  for (subcategory in subcategories_in_cluster) {\n    if (subcategory %in% names(ts_list)) {\n      ts_current <- ts_list[[subcategory]]  # Access the time series\n\n      # Split the data into training and validation sets (70% training, 30% testing)\n      train_size <- floor(0.7 * length(ts_current))\n      train_ts <- window(ts_current, end = c(2014 + (train_size - 1) %/% 12, (train_size - 1) %% 12 + 1))\n      test_ts <- window(ts_current, start = c(2014 + train_size %/% 12, train_size %% 12 + 1))\n\n      # 1. ARIMA\n      arima_model <- auto.arima(train_ts)\n      arima_forecast <- forecast(arima_model, h = length(test_ts))\n      arima_accuracy <- accuracy(arima_forecast, test_ts)\n\n      # 2. Holt-Winters\n      hw_model <- HoltWinters(train_ts)\n      hw_forecast <- forecast(hw_model, h = length(test_ts))\n      hw_accuracy <- accuracy(hw_forecast, test_ts)\n\n      # 3. ETS\n      ets_model <- ets(train_ts)\n      ets_forecast <- forecast(ets_model, h = length(test_ts))\n      ets_accuracy <- accuracy(ets_forecast, test_ts)\n\n      # Store results for the subcategory\n      cluster_forecast_results[[subcategory]] <- list(\n        ARIMA = list(Model = arima_model, Forecast = arima_forecast, Accuracy = arima_accuracy),\n        HoltWinters = list(Model = hw_model, Forecast = hw_forecast, Accuracy = hw_accuracy),\n        ETS = list(Model = ets_model, Forecast = ets_forecast, Accuracy = ets_accuracy)\n      )\n    } else {\n      cat(\"\\nSub-Category not found in ts_list:\", subcategory, \"\\n\")\n    }\n  }\n\n  # Store results for the cluster\n  forecast_results_by_cluster[[paste0(\"Cluster_\", cluster_id)]] <- cluster_forecast_results\n}\n\n# Step 5: Print Forecasting Accuracy for Each Cluster\nfor (cluster_id in names(forecast_results_by_cluster)) {\n  cat(\"\\n\\nResults for\", cluster_id, \"\\n\")\n  cluster_results <- forecast_results_by_cluster[[cluster_id]]\n\n  for (subcategory in names(cluster_results)) {\n    cat(\"\\nSub-Category:\", subcategory, \"\\n\")\n\n    cat(\"\\nARIMA Accuracy:\\n\")\n    print(cluster_results[[subcategory]]$ARIMA$Accuracy)\n\n    cat(\"\\nHolt-Winters Accuracy:\\n\")\n    print(cluster_results[[subcategory]]$HoltWinters$Accuracy)\n\n    cat(\"\\nETS Accuracy:\\n\")\n    print(cluster_results[[subcategory]]$ETS$Accuracy)\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\nResults for Cluster_1 \n\nSub-Category: Binders \n\nARIMA Accuracy:\n                    ME      RMSE      MAE        MPE     MAPE      MASE\nTraining set 0.7706014  4.643476 2.982256  0.6865304 12.91204 0.4854835\nTest set     5.9407398 10.783528 7.616473 10.3681817 17.32927 1.2398909\n                   ACF1 Theil's U\nTraining set 0.04429472        NA\nTest set     0.04929320 0.3573866\n\nHolt-Winters Accuracy:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 0.8668058 5.215491 3.789990 -0.4040374 15.36545 0.6169751\nTest set     2.2473496 8.712049 6.243226  0.1597635 16.02219 1.0163391\n                     ACF1 Theil's U\nTraining set -0.389045966        NA\nTest set     -0.001830843 0.2777843\n\nETS Accuracy:\n                   ME      RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.743354  4.712561 3.656409  1.358692 15.41708 0.5952293\nTest set     7.439784 12.216161 8.825094 10.667088 20.93561 1.4366433\n                   ACF1 Theil's U\nTraining set -0.2225537        NA\nTest set      0.0608554 0.3767484\n\n\nResults for Cluster_2 \n\nSub-Category: Paper \n\nARIMA Accuracy:\n                    ME      RMSE       MAE        MPE     MAPE      MASE\nTraining set  1.117384  6.309464  3.827714   1.702165 14.32157 0.5468163\nTest set     -9.014128 12.230774 10.375521 -30.103886 31.89519 1.4822173\n                     ACF1 Theil's U\nTraining set -0.007064333        NA\nTest set      0.108516273 0.6984566\n\nHolt-Winters Accuracy:\n                     ME     RMSE       MAE        MPE     MAPE      MASE\nTraining set   1.509544  7.27986  4.854547   3.325281 16.41309 0.6935067\nTest set     -12.036865 14.48061 13.347292 -39.791842 41.51609 1.9067560\n                    ACF1 Theil's U\nTraining set -0.02710216        NA\nTest set      0.10006713  0.845232\n\nETS Accuracy:\n                     ME      RMSE      MAE         MPE     MAPE      MASE\nTraining set 0.60941117  6.204628 4.474990 -1.66197953 18.73240 0.6392842\nTest set     0.08500424 11.304488 8.247017 -0.08462612 20.47598 1.1781453\n                    ACF1 Theil's U\nTraining set 0.005205508        NA\nTest set     0.341519997  0.582549\n\n\nResults for Cluster_3 \n\nSub-Category: Furnishings \n\nARIMA Accuracy:\n                    ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.0050974 3.873555 2.567868 -6.624673 20.20958 0.5559302\nTest set     3.9523810 7.782677 6.238095 10.720079 22.75340 1.3505155\n                    ACF1 Theil's U\nTraining set -0.20160077        NA\nTest set     -0.03570035 0.6102339\n\nHolt-Winters Accuracy:\n                    ME     RMSE      MAE      MPE     MAPE      MASE\nTraining set 0.9137655 4.164677 3.475419 7.490317 24.75583 0.7524103\nTest set     3.6371987 7.200985 5.673333 9.724318 20.50134 1.2282474\n                    ACF1 Theil's U\nTraining set -0.43317305        NA\nTest set      0.01804785 0.5689788\n\nETS Accuracy:\n                    ME     RMSE      MAE        MPE     MAPE     MASE\nTraining set 0.7370579 3.466690 2.851745  0.4301648 20.85220 0.617388\nTest set     6.0973038 8.354163 6.690832 20.7276401 23.55317 1.448531\n                   ACF1 Theil's U\nTraining set -0.2087083        NA\nTest set      0.3729315 0.7455977\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n## Step 6: Visualize the Clusters\nlibrary(ggplot2)\n\nggplot(time_series_features, aes(x = TrendStrength, y = SeasonalStrength, color = as.factor(Cluster))) +\n  geom_point(size = 3) +\n  labs(title = \"Clusters of Subcategories Based on Time-Series Features\",\n       x = \"Trend Strength\", y = \"Seasonal Strength\", color = \"Cluster\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-docx/somelable-1.png)\n:::\n\n```{.r .cell-code .hidden}\n#\n#check\n#residual diagnostic\nfor (cluster_id in names(forecast_results_by_cluster)) {\n  cluster_results <- forecast_results_by_cluster[[cluster_id]]\n  for (subcategory in names(cluster_results)) {\n    cat(\"\\nResidual Diagnostics for Sub-Category:\", subcategory, \"\\n\")\n    checkresiduals(cluster_results[[subcategory]]$ARIMA$Model)\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nResidual Diagnostics for Sub-Category: Binders \n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-docx/somelable-2.png)\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tLjung-Box test\n\ndata:  Residuals from ARIMA(0,1,2)(0,1,0)[12]\nQ* = 2.6295, df = 5, p-value = 0.7569\n\nModel df: 2.   Total lags used: 7\n\n\nResidual Diagnostics for Sub-Category: Paper \n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-docx/somelable-3.png)\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tLjung-Box test\n\ndata:  Residuals from ARIMA(0,1,1)(0,1,0)[12]\nQ* = 6.6676, df = 6, p-value = 0.3527\n\nModel df: 1.   Total lags used: 7\n\n\nResidual Diagnostics for Sub-Category: Furnishings \n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-docx/somelable-4.png)\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tLjung-Box test\n\ndata:  Residuals from ARIMA(0,0,0)(0,1,0)[12] with drift\nQ* = 9.5952, df = 7, p-value = 0.2127\n\nModel df: 0.   Total lags used: 7\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# P-value is higher then 0.1 so we have stationary data, this is good\n#cluster level metrics\ncluster_metrics <- data.frame()\nfor (cluster_id in names(forecast_results_by_cluster)) {\n  cluster_results <- forecast_results_by_cluster[[cluster_id]]\n  cluster_rmse <- sapply(cluster_results, function(x) x$ARIMA$Accuracy[\"Test set\", \"RMSE\"])\n  cluster_mape <- sapply(cluster_results, function(x) x$ARIMA$Accuracy[\"Test set\", \"MAPE\"])\n  cluster_metrics <- rbind(cluster_metrics, data.frame(Cluster = cluster_id, MeanRMSE = mean(cluster_rmse), MeanMAPE = mean(cluster_mape)))\n}\nprint(cluster_metrics)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Cluster  MeanRMSE MeanMAPE\n1 Cluster_1 10.783528 17.32927\n2 Cluster_2 12.230774 31.89519\n3 Cluster_3  7.782677 22.75340\n```\n\n\n:::\n:::\n\n\nCluster 1 (e.g., Binders): ARIMA outperformed other methods due to significant autocorrelation and trend components.\n\nCluster 2 (e.g., Furnishings): ETS was the most accurate method, effectively balancing trend and seasonality.\n\nCluster 3 (e.g., Paper): ETS also performed best, with ARIMA showing higher error rates due to variability in random components.\n\nResidual diagnostics were performed for all ARIMA models, confirming no significant autocorrelation (p > 0.05).\n\nCluster-Level Metrics based on mean RMSE and MAPE show:\n-   Cluster 1 had the lowest RMSE using ARIMA.\n-   Cluster 2 and 3 were better modeled with ETS\n\n### Conclusion (4b)\n\nClustering allows for tailored forecasting strategies. We conclude that for the given data set ARIMA is more effective for clusters with strong trends, while ETS is preferable for clusters with mixed seasonal and trend characteristics. The approach aligns with lecture notes, emphasizing the importance of adapting models based on time series characteristics.\n\n## Forecasting future values\n\n### Forecasting 3 products (5a)\n\nIn this session, we focused on evaluating different forecasting models (ARIMA, Holt-Winters, and ETS) for multiple sub-categories by analyzing their accuracy metrics, such as RMSE, MAPE, and residual diagnostics. Based on the evaluation results, we selected the best-performing model for each sub-category. We then used these models to forecast the future outcomes for each sub-category, projecting the data for the next year. Note: we may need to interpret the outcomes and explain why we pick the certain model\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n#Binders->choose ARIMA\nbinders_ts <- ts_list[[\"Binders\"]]\narima_model <- auto.arima(binders_ts)\nsummary(arima_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: binders_ts \nARIMA(1,1,1)(0,1,0)[12] \n\nCoefficients:\n          ar1      ma1\n      -0.4781  -0.4819\ns.e.   0.2324   0.2426\n\nsigma^2 = 51.18:  log likelihood = -117.97\nAIC=241.94   AICc=242.72   BIC=246.61\n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE     MAPE     MASE        ACF1\nTraining set 0.864453 5.931761 4.092168 -1.363101 15.06142 0.558023 -0.03746012\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\narima_forecast <- forecast(arima_model, h = 12)\nprint(arima_forecast)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Point Forecast    Lo 80     Hi 80     Lo 95     Hi 95\nJan 2018       32.48390 23.31571  41.65208 18.462370  46.50543\nFeb 2018       23.77181 14.59632  32.94731  9.739103  37.80452\nMar 2018       45.85265 35.59989  56.10541 30.172412  61.53289\nApr 2018       46.20475 35.63662  56.77287 30.042185  62.36730\nMay 2018       44.08009 32.93968  55.22051 27.042295  61.11789\nJun 2018       44.61784 33.06359  56.17210 26.947139  62.28855\nJul 2018       40.36072 28.34867  52.37277 21.989869  58.73157\nAug 2018       44.48366 32.05796  56.90937 25.480189  63.48714\nSep 2018       73.42488 60.58630  86.26346 53.789962  93.05979\nOct 2018       51.45299 38.22024  64.68573 31.215253  71.69072\nNov 2018       72.43955 58.82134  86.05775 51.612293  93.26680\nDec 2018       89.44597 75.45417 103.43777 68.047363 110.84458\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nplot(arima_forecast, main = \"ARIMA Forecast for Binders (Next 12 Months)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-docx/unnamed-chunk-18-1.png)\n:::\n\n```{.r .cell-code .hidden}\n#Paper->choose ETS\npaper_ts <- ts_list[[\"Paper\"]]\nets_model <- ets(paper_ts)\nsummary(ets_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nETS(M,N,A) \n\nCall:\nets(y = paper_ts)\n\n  Smoothing parameters:\n    alpha = 0.3075 \n    gamma = 1e-04 \n\n  Initial states:\n    l = 22.5954 \n    s = 17.4472 16.5763 -4.1253 15.2986 0.421 -5.102\n           -0.6145 -0.0341 -7.985 -2.6766 -15.6576 -13.5481\n\n  sigma:  0.2365\n\n     AIC     AICc      BIC \n365.1517 380.1517 393.2197 \n\nTraining set error measures:\n                   ME     RMSE      MAE     MPE     MAPE      MASE       ACF1\nTraining set 1.450303 6.386648 4.166875 1.75373 14.03399 0.5245018 0.03600045\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nets_forecast <- forecast(ets_model, h = 12)\nprint(ets_forecast)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\nJan 2018       30.45588 21.22661 39.68516 16.34092 44.57085\nFeb 2018       28.34651 19.27484 37.41819 14.47258 42.22044\nMar 2018       41.32776 28.18367 54.47185 21.22561 61.42990\nApr 2018       36.01933 23.73891 48.29976 17.23804 54.80062\nMay 2018       43.97017 29.09477 58.84557 21.22021 66.72013\nJun 2018       43.39031 28.07408 58.70653 19.96616 66.81445\nJul 2018       38.90220 24.12853 53.67588 16.30782 61.49659\nAug 2018       44.42410 27.84663 61.00158 19.07104 69.77717\nSep 2018       59.30522 38.44478 80.16566 27.40193 91.20850\nOct 2018       39.87917 22.81850 56.93984 13.78712 65.97122\nNov 2018       60.58102 38.27853 82.88352 26.47230 94.68975\nDec 2018       61.45110 38.17748 84.72473 25.85717 97.04504\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nplot(ets_forecast, main = \"ETS Forecast for Paper (Next 12 Months)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-docx/unnamed-chunk-18-2.png)\n:::\n\n```{.r .cell-code .hidden}\n#Furnishings->choose ETS\nfurnishings_ts <- ts_list[[\"Furnishings\"]]\nets_model <- ets(furnishings_ts)\nsummary(ets_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nETS(M,A,A) \n\nCall:\nets(y = furnishings_ts)\n\n  Smoothing parameters:\n    alpha = 0.0438 \n    beta  = 0.0437 \n    gamma = 2e-04 \n\n  Initial states:\n    l = 15.4275 \n    b = -0.1137 \n    s = 13.3158 15.6269 -2.2962 10.1503 -5.0017 -2.448\n           -3.4406 -1.0728 -1.1262 -4.3688 -11.689 -7.6497\n\n  sigma:  0.2527\n\n     AIC     AICc      BIC \n338.8888 359.2888 370.6992 \n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE    MAPE      MASE\nTraining set 0.6402485 3.793384 2.884208 -0.8302416 16.2441 0.5352139\n                   ACF1\nTraining set 0.04613441\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nets_forecast <- forecast(ets_model, h = 12)\nprint(ets_forecast)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Point Forecast    Lo 80    Hi 80     Lo 95    Hi 95\nJan 2018       21.37433 14.45350 28.29515 10.789837 31.95882\nFeb 2018       18.56644 12.52244 24.61044  9.322944 27.80994\nMar 2018       27.11574 18.26943 35.96205 13.586481 40.64500\nApr 2018       31.58782 21.22159 41.95404 15.734048 47.44158\nMay 2018       32.87189 21.95559 43.78819 16.176845 49.56693\nJun 2018       31.73384 20.95052 42.51717 15.242170 48.22551\nJul 2018       33.95710 22.18459 45.72960 15.952604 51.96159\nAug 2018       32.63192 20.83816 44.42569 14.594916 50.66893\nSep 2018       49.01546 31.92061 66.11032 22.871140 75.15979\nOct 2018       37.79884 23.38308 52.21460 15.751844 59.84584\nNov 2018       56.95159 36.44698 77.45621 25.592490 88.31070\nDec 2018       55.87232 34.96477 76.77986 23.896983 87.84765\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nplot(ets_forecast, main = \"ETS Forecast for Furnishings (Next 12 Months)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-docx/unnamed-chunk-18-3.png)\n:::\n:::\n\n\n\n### Applying to all data (5b)\n\nIn this session, we first grouped the sub-categories into clusters based on key time-series features, including trend strength, seasonal strength, and random strength, using hierarchical clustering. Once the clusters were formed, we applied and evaluated multiple forecasting models—ARIMA, Holt-Winters, and ETS—on each sub-category within its respective cluster, comparing their accuracy metrics such as RMSE and MAPE. Based on the evaluation results, we selected the best-performing model for each sub-category and used it to forecast the future outcomes within a year, leveraging the clustering to enhance the accuracy and relevance of our predictions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n#Cluster_Binders->Holt-Winters\ncluster_id <- 1\nsubcategory <- \"Binders\"\nhw_model <- forecast_results_by_cluster[[paste0(\"Cluster_\", cluster_id)]][[subcategory]]$HoltWinters$Model\nhw_forecast <- forecast(hw_model, h = 12)\nprint(hw_forecast)\nplot(hw_forecast, main = \"Holt-Winters Forecast for Binders (Next 12 Months)\", xlab = \"Time\", ylab = \"Forecasted Values\")\n\n#Cluster_paper->ETS\ncluster_id <- 2\nsubcategory <- \"Paper\"\nets_model <- forecast_results_by_cluster[[paste0(\"Cluster_\", cluster_id)]][[subcategory]]$ETS$Model\nets_forecast <- forecast(ets_model, h = 12)\nprint(ets_forecast)\nplot(ets_forecast, main = \"ETS Forecast for Paper (Next 12 Months)\", xlab = \"Time\", ylab = \"Forecasted Values\")\n\n#Cluster_Furnishings->Holt-Winters\ncluster_id <- 3\nsubcategory <- \"Furnishings\"\nhw_model <- forecast_results_by_cluster[[paste0(\"Cluster_\", cluster_id)]][[subcategory]]$HoltWinters$Model\nhw_forecast <- forecast(hw_model, h = 12)\nprint(hw_forecast)\nplot(hw_forecast, main = \"Holt-Winters Forecast for Furnishings (Next 12 Months)\", xlab = \"Time\", ylab = \"Forecasted Values\")\n```\n:::\n\n\n## Forecast interpretation\n\nLorem Ipsum\n\n\n\n\n\n\n<br>\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# Check for missing values\nmissing_values <- colSums(is.na(data))\nprint(missing_values)  # Print missing values for reference\n# heat map\nlibrary(Amelia)\nmissmap(data, main = \"Missing Data Pattern\")\n#distribution of key variables\n#plot Quantity\nggplot(data, aes(x = Quantity)) +\n  geom_histogram(binwidth = 1, fill = \"steelblue\") +\n  labs(title = \"Distribution of Quantity\", x = \"Quantity\", y = \"Frequency\") +\n  theme_minimal()\n#plot sales\nggplot(data, aes(x = Sales)) +\n  geom_histogram(binwidth = 50, fill = \"tomato\") +\n  labs(title = \"Distribution of Sales\", x = \"Sales\", y = \"Frequency\") +\n  theme_minimal()\n# plot profit\nggplot(data, aes(x = Profit)) +\n  geom_histogram(binwidth = 10, fill = \"purple\") +\n  labs(title = \"Distribution of Profit\", x = \"Profit\", y = \"Frequency\") +\n  theme_minimal()\n# time based trends\ndata$Order_Date <- as.Date(data$Order_Date, format = \"%Y-%m-%d\")  # Ensure date format\ntime_series <- data %>%\n  group_by(Order_Date) %>%\n  summarize(total_sales = sum(Sales), total_profit = sum(Profit), total_quantity = sum(Quantity))\n\nggplot(time_series, aes(x = Order_Date)) +\n  geom_line(aes(y = total_sales, color = \"Sales\")) +\n  geom_line(aes(y = total_profit, color = \"Profit\")) +\n  geom_line(aes(y = total_quantity, color = \"Quantity\")) +\n  labs(title = \"Sales, Profit, and Quantity Over Time\", x = \"Date\", y = \"Value\") +\n  theme_minimal() +\n  scale_color_manual(name = \"Metrics\", values = c(\"Sales\" = \"blue\", \"Profit\" = \"green\", \"Quantity\" = \"red\"))\n\n#sales by category and sub category\ncategory_sales <- data %>%\n  group_by(Category, Sub_Category) %>%\n  summarize(total_sales = sum(Sales))\n\nggplot(category_sales, aes(x = reorder(Sub_Category, -total_sales), y = total_sales, fill = Category)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Sales by Category and Sub-Category\", x = \"Sub-Category\", y = \"Total Sales\") +\n  theme_minimal() +\n  coord_flip()\n\n#Outliers detection\n#Quantity\nggplot(data, aes(x = Category, y = Quantity)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Quantity by Category\", x = \"Category\", y = \"Quantity\")\n#sales\nggplot(data, aes(x = Category, y = Sales)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Sales by Category\", x = \"Category\", y = \"Sales\")\n\n#profit\nggplot(data, aes(x = Category, y = Profit)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Profit by Category\", x = \"Category\", y = \"Profit\")\n#Geo visualization\n\nus_map <- map_data(\"state\")\nif(\"State\" %in% colnames(data)) {\n  state_sales <- data %>%\n    group_by(State) %>%\n    summarize(total_sales = sum(Sales))\n\n  # Convert state names to lowercase to match map data\n  state_sales$State <- tolower(state_sales$State)\n\n  # Merge state sales data with map data\n  state_sales_map <- merge(us_map, state_sales, by.x = \"region\", by.y = \"State\", all.x = TRUE)\n\n  # Plot sales by state\n  ggplot(state_sales_map, aes(long, lat, group = group, fill = total_sales)) +\n    geom_polygon(color = \"white\") +\n    scale_fill_continuous(low = \"lightblue\", high = \"darkblue\", na.value = \"gray90\") +\n    labs(title = \"Sales by State\", fill = \"Total Sales\") +\n    theme_void() +\n    coord_fixed(1.3)\n}\n\n# correlation matrix\nnumerical_data <- data %>% select(where(is.numeric))\n\ncor_matrix <- cor(numerical_data, use = \"complete.obs\")\n\n# Convert the correlation matrix to a long format\ncor_data <- as.data.frame(as.table(cor_matrix))\n\n# Plot the correlation matrix using ggplot2\nggplot(cor_data, aes(Var1, Var2, fill = Freq)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\",\n                       midpoint = 0, limit = c(-1, 1), space = \"Lab\",\n                       name=\"Correlation\") +\n  geom_text(aes(label = round(Freq, 2)), color = \"black\", size = 4) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1,\n                                   size = 12, hjust = 1)) +\n  coord_fixed() +\n  labs(title = \"Correlation Matrix of Key Variables\", x = \"\", y = \"\")\n```\n:::\n\n\n\n\n### Forecasting??\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n#Aggregate sales per month\nmonthly_sales <- data %>%\n  mutate(Month = floor_date(Order_Date, \"month\")) %>%\n  group_by(Month) %>%\n  summarize(total_sales = sum(Sales))\n#Convert to time series\nsales_ts <- ts(monthly_sales$total_sales, frequency = 12, start = c(year(min(monthly_sales$Month)), month(min(monthly_sales$Month))))\n#Arima model\narima_model <- auto.arima(sales_ts)\narima_forecast <- forecast(arima_model, h = 12)\nautoplot(arima_forecast) + labs(title = \"ARIMA Forecast for Monthly Sales\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-docx/unnamed-chunk-21-1.png)\n:::\n\n```{.r .cell-code .hidden}\n#Holts winter model\nhw_model <- HoltWinters(sales_ts)\nhw_forecast <- forecast(hw_model, h = 12)\nautoplot(hw_forecast) + labs(title = \"Holt-Winters Forecast for Monthly Sales\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-docx/unnamed-chunk-21-2.png)\n:::\n\n```{.r .cell-code .hidden}\n# clustering for segmentation\nlibrary(cluster)\n#data clustering\nclustering_data <- data %>%\n  select(Sales, Quantity, Discount, Profit) %>%\n  na.omit()\nset.seed(123)\nkmeans_model <- kmeans(clustering_data, centers = 3)\ndata$Cluster <- as.factor(kmeans_model$cluster)\n# visualize clustering result\nggplot(data, aes(x = Sales, y = Profit, color = Cluster)) +\n  geom_point(alpha = 0.6) +\n  labs(title = \"K-Means Clustering of Sales and Profit\", x = \"Sales\", y = \"Profit\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-docx/unnamed-chunk-21-3.png)\n:::\n:::",
    "supporting": [
      "index_files/figure-docx"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}