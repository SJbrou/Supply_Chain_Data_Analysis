<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving
and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">

<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">

<front>


<article-meta>


<title-group>
<article-title>Supply Chain Data Analytics</article-title>
<subtitle>Analyzing and Forcasting Supermarket Sales</subtitle>
</title-group>

<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>(2671939)</surname>
<given-names>Stan Brouwer</given-names>
</name>
<string-name>Stan Brouwer (2671939)</string-name>

<xref ref-type="aff" rid="aff-1">a</xref>
<xref ref-type="corresp" rid="cor-1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>(2840693)</surname>
<given-names>Liz Chen</given-names>
</name>
<string-name>Liz Chen (2840693)</string-name>

<xref ref-type="aff" rid="aff-2">b</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>(2854979)</surname>
<given-names>Maaike Lamberts</given-names>
</name>
<string-name>Maaike Lamberts (2854979)</string-name>

<xref ref-type="aff" rid="aff-3">c</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Schroor</surname>
<given-names>Niek</given-names>
</name>
<string-name>Niek Schroor</string-name>

<xref ref-type="aff" rid="aff-4">d</xref>
</contrib>
</contrib-group>
<aff id="aff-1">
<institution-wrap>
<institution>Vrije Universiteit</institution>
</institution-wrap>







</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Master TSCM</institution>
</institution-wrap>







</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Supply Chain Data analysis</institution>
</institution-wrap>







</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Group 10</institution>
</institution-wrap>







</aff>
<author-notes>
<corresp id="cor-1"></corresp>
</author-notes>









<history></history>






</article-meta>

</front>

<body>
<sec id="data-selection">
  <title>1 Data selection</title>
  <p>We analyze, forecast and interpret the
  <ext-link ext-link-type="uri" xlink:href="https://public.tableau.com/app/sample-data/sample_-_superstore.xls">Superstore
  sales</ext-link> provided by
  <ext-link ext-link-type="uri" xlink:href="https://public.tableau.com/app/learn/sample-data">Tableau</ext-link>
  using different statistical and machine learning methods.</p>
  <p>The dataset provided contains information about products, sales and
  profits of a fictitious US company. The dataset contains about 10,000
  rows with 1,850 unique product names and 17 product subcategories,
  covering four consecutive years on a daily basis.</p>
  <p>We describe our work in the PDF version. However, we would like to
  recommend reading our quarto manuscript <italic>here</italic> as it
  contains the <bold>relevant</bold> R code in the Article Notebook.</p>
</sec>
<sec id="data-pre-processing">
  <title>2 Data Pre-processing</title>
  <p>The superstore data set we selected is of high quality: At first
  glance (which needs to be verified during the visualization), the data
  appears to have been recorded regularly and without interruptions.
  There is no sign of a sudden structural change. Since the data are
  consumer products, it should contain both trends and seasonality.
  Nevertheless, we have included hypothetical steps to demonstrate our
  understanding of the data preprocessing procedure. In detail, we
  did:</p>
  <list list-type="bullet">
    <list-item>
      <p>Remove whitespaces from column names</p>
    </list-item>
    <list-item>
      <p>Remove the Row_ID column as it can be inferred by it’s
      index</p>
    </list-item>
    <list-item>
      <p>Remove all columns with a single unique value, as storing these
      would be
      <ext-link ext-link-type="uri" xlink:href="https://few.vu.nl/~molenaar/courses/StatR/chapters/B-06-raw_data.html">redundant</ext-link></p>
    </list-item>
    <list-item>
      <p>Ensure machine-readable date formats in yyyy-mm-dd as these
      usually differ per locale.</p>
    </list-item>
    <list-item>
      <p>Ensure proper decimal separators</p>
    </list-item>
    <list-item>
      <p>Calculate the number of missing values (both NA and empty
      string ““) per column.</p>
    </list-item>
  </list>
  <p>After these steps (and transposing the table for better document
  formatting), the data looks as follows:</p>
  <table-wrap>
    <caption>
      <p>First 3 Rows of the Data (Transposed)</p>
    </caption>
    <table>
      <colgroup>
        <col width="8%" />
        <col width="20%" />
        <col width="36%" />
        <col width="35%" />
      </colgroup>
      <tbody>
        <tr>
          <td align="left">Order_ID</td>
          <td align="left">CA-2016-152156</td>
          <td align="left">CA-2016-152156</td>
          <td align="left">CA-2016-138688</td>
        </tr>
        <tr>
          <td align="left">Order_Date</td>
          <td align="left">2016-11-08</td>
          <td align="left">2016-11-08</td>
          <td align="left">2016-06-12</td>
        </tr>
        <tr>
          <td align="left">Ship_Date</td>
          <td align="left">2016-11-11</td>
          <td align="left">2016-11-11</td>
          <td align="left">2016-06-16</td>
        </tr>
        <tr>
          <td align="left">Ship_Mode</td>
          <td align="left">Second Class</td>
          <td align="left">Second Class</td>
          <td align="left">Second Class</td>
        </tr>
        <tr>
          <td align="left">Customer_ID</td>
          <td align="left">CG-12520</td>
          <td align="left">CG-12520</td>
          <td align="left">DV-13045</td>
        </tr>
        <tr>
          <td align="left">Customer_Name</td>
          <td align="left">Claire Gute</td>
          <td align="left">Claire Gute</td>
          <td align="left">Darrin Van Huff</td>
        </tr>
        <tr>
          <td align="left">Segment</td>
          <td align="left">Consumer</td>
          <td align="left">Consumer</td>
          <td align="left">Corporate</td>
        </tr>
        <tr>
          <td align="left">City</td>
          <td align="left">Henderson</td>
          <td align="left">Henderson</td>
          <td align="left">Los Angeles</td>
        </tr>
        <tr>
          <td align="left">State</td>
          <td align="left">Kentucky</td>
          <td align="left">Kentucky</td>
          <td align="left">California</td>
        </tr>
        <tr>
          <td align="left">Postal_Code</td>
          <td align="left">42420</td>
          <td align="left">42420</td>
          <td align="left">90036</td>
        </tr>
        <tr>
          <td align="left">Region</td>
          <td align="left">South</td>
          <td align="left">South</td>
          <td align="left">West</td>
        </tr>
        <tr>
          <td align="left">Product_ID</td>
          <td align="left">FUR-BO-10001798</td>
          <td align="left">FUR-CH-10000454</td>
          <td align="left">OFF-LA-10000240</td>
        </tr>
        <tr>
          <td align="left">Category</td>
          <td align="left">Furniture</td>
          <td align="left">Furniture</td>
          <td align="left">Office Supplies</td>
        </tr>
        <tr>
          <td align="left">Sub_Category</td>
          <td align="left">Bookcases</td>
          <td align="left">Chairs</td>
          <td align="left">Labels</td>
        </tr>
        <tr>
          <td align="left">Product_Name</td>
          <td align="left">Bush Somerset Collection Bookcase</td>
          <td align="left">Hon Deluxe Fabric Upholstered Stacking
          Chairs, Rounded Back</td>
          <td align="left">Self-Adhesive Address Labels for Typewriters
          by Universal</td>
        </tr>
        <tr>
          <td align="left">Sales</td>
          <td align="left">261.96</td>
          <td align="left">731.94</td>
          <td align="left">14.62</td>
        </tr>
        <tr>
          <td align="left">Quantity</td>
          <td align="left">2</td>
          <td align="left">3</td>
          <td align="left">2</td>
        </tr>
        <tr>
          <td align="left">Discount</td>
          <td align="left">0</td>
          <td align="left">0</td>
          <td align="left">0</td>
        </tr>
        <tr>
          <td align="left">Profit</td>
          <td align="left">41.9136</td>
          <td align="left">219.5820</td>
          <td align="left">6.8714</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <p>We did not find any missing values, confirming the quality of the
  data set. There is some more processing to do, for instance the
  removal of outliers. However, by doing so we impose our own
  assumptions on the data. Let’s start by evaluating the descriptive
  statistics of our data and check if further processing is
  required.</p>
  <table-wrap>
    <caption>
      <p>Descriptive Statistics for Numeric Columns</p>
    </caption>
    <table>
      <thead>
        <tr>
          <th align="left">Column</th>
          <th align="left">Min</th>
          <th align="left">Max</th>
          <th align="left">Mean</th>
          <th align="left">Median</th>
          <th align="left">StdDev</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left">Postal_Code</td>
          <td align="left">1040</td>
          <td align="left">99301</td>
          <td align="left">55190.38</td>
          <td align="left">56430.5</td>
          <td align="left">32063.69</td>
        </tr>
        <tr>
          <td align="left">Sales</td>
          <td align="left">0.444</td>
          <td align="left">22638.48</td>
          <td align="left">229.858</td>
          <td align="left">54.49</td>
          <td align="left">623.2451</td>
        </tr>
        <tr>
          <td align="left">Quantity</td>
          <td align="left">1</td>
          <td align="left">14</td>
          <td align="left">3.789574</td>
          <td align="left">3</td>
          <td align="left">2.22511</td>
        </tr>
        <tr>
          <td align="left">Discount</td>
          <td align="left">0</td>
          <td align="left">0.8</td>
          <td align="left">0.1562027</td>
          <td align="left">0.2</td>
          <td align="left">0.206452</td>
        </tr>
        <tr>
          <td align="left">Profit</td>
          <td align="left">-6599.978</td>
          <td align="left">8399.976</td>
          <td align="left">28.6569</td>
          <td align="left">8.6665</td>
          <td align="left">234.2601</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap>
    <caption>
      <p>Descriptive Statistics for Date Columns</p>
    </caption>
    <table>
      <thead>
        <tr>
          <th align="left">Column</th>
          <th align="left">Earliest</th>
          <th align="left">Latest</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left">Order_Date</td>
          <td align="left">2014-01-03</td>
          <td align="left">2017-12-30</td>
        </tr>
        <tr>
          <td align="left">Ship_Date</td>
          <td align="left">2014-01-07</td>
          <td align="left">2018-01-05</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <p>We inspect the orders with the lowest and highest Sales amount (in
  USD). The most expensive orders were professional printers, cameras
  and teleconferencing units with high unit prices. The orders with the
  lowest sales amount were often binders and had a high Discount
  rate.</p>
  <p>Interestingly there are orders with a negative profit. They
  typically have high Discount rates and often concern the same item,
  such as the “Cubify CubeX 3D Printer Triple Head Print”. The orders
  with a negative Profit were often part of a larger order (for instance
  CA-2016-108196), and placed by customers with multiple orders. We
  suspect these negative Profit’s to be caused by items of lower quality
  that receive discounts, general discount codes, or volume discounts.
  However, due to the high discounts especially on orders with negative
  profit, we assume these to be valid orders.</p>
  <p>** Some negative profit products **</p>
  <p>In figure x we plotted the quantities of the most sold products.
  Unfortunately, the sold quantities of individual products were too low
  to determine any meaningful trends.</p>
  <fig>
    <caption><p>Figure X Sale quantity of the most popular
    products</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/Top_Products_Quantity-1.png" />
  </fig>
  <p>Our proposed workaround is to aggregate Product_Name by
  Sub_Category, and treat it as a single product for the rest of the
  assignment, which we plotted in figure X.</p>
  <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/Aggregated_Sub_Category_sales-1.png" />
  <p>This aggregated Quantity starts to show trends and seasonality, and
  is much more useful to base predictions on! We will use these
  aggregated sub-categories for the rest of the assignment.</p>
  <p>To properly finish our data preprocessing we ran some statistics on
  Quantity aggregated by Sub_Category. Table x contains some descriptive
  statistics.</p>
  <table-wrap>
    <caption>
      <p>Statistics for Sub_Category quantity</p>
    </caption>
    <table>
      <thead>
        <tr>
          <th align="left">Sub_Category</th>
          <th align="right">Min</th>
          <th align="right">Mean</th>
          <th align="right">Max</th>
          <th align="right">Sd</th>
          <th align="right">CI_lower</th>
          <th align="right">CI_upper</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left">Accessories</td>
          <td align="right">1</td>
          <td align="right">3.84</td>
          <td align="right">14</td>
          <td align="right">2.28</td>
          <td align="right">3.68</td>
          <td align="right">4.00</td>
        </tr>
        <tr>
          <td align="left">Appliances</td>
          <td align="right">1</td>
          <td align="right">3.71</td>
          <td align="right">14</td>
          <td align="right">2.12</td>
          <td align="right">3.52</td>
          <td align="right">3.90</td>
        </tr>
        <tr>
          <td align="left">Art</td>
          <td align="right">1</td>
          <td align="right">3.77</td>
          <td align="right">14</td>
          <td align="right">2.13</td>
          <td align="right">3.62</td>
          <td align="right">3.92</td>
        </tr>
        <tr>
          <td align="left">Binders</td>
          <td align="right">1</td>
          <td align="right">3.92</td>
          <td align="right">14</td>
          <td align="right">2.29</td>
          <td align="right">3.80</td>
          <td align="right">4.04</td>
        </tr>
        <tr>
          <td align="left">Bookcases</td>
          <td align="right">1</td>
          <td align="right">3.81</td>
          <td align="right">13</td>
          <td align="right">2.28</td>
          <td align="right">3.51</td>
          <td align="right">4.11</td>
        </tr>
        <tr>
          <td align="left">Chairs</td>
          <td align="right">1</td>
          <td align="right">3.82</td>
          <td align="right">14</td>
          <td align="right">2.28</td>
          <td align="right">3.64</td>
          <td align="right">4.00</td>
        </tr>
        <tr>
          <td align="left">Copiers</td>
          <td align="right">1</td>
          <td align="right">3.44</td>
          <td align="right">9</td>
          <td align="right">1.83</td>
          <td align="right">3.01</td>
          <td align="right">3.87</td>
        </tr>
        <tr>
          <td align="left">Envelopes</td>
          <td align="right">1</td>
          <td align="right">3.57</td>
          <td align="right">9</td>
          <td align="right">2.05</td>
          <td align="right">3.32</td>
          <td align="right">3.82</td>
        </tr>
        <tr>
          <td align="left">Fasteners</td>
          <td align="right">1</td>
          <td align="right">4.21</td>
          <td align="right">14</td>
          <td align="right">2.41</td>
          <td align="right">3.89</td>
          <td align="right">4.53</td>
        </tr>
        <tr>
          <td align="left">Furnishings</td>
          <td align="right">1</td>
          <td align="right">3.72</td>
          <td align="right">14</td>
          <td align="right">2.16</td>
          <td align="right">3.58</td>
          <td align="right">3.86</td>
        </tr>
        <tr>
          <td align="left">Labels</td>
          <td align="right">1</td>
          <td align="right">3.85</td>
          <td align="right">14</td>
          <td align="right">2.35</td>
          <td align="right">3.61</td>
          <td align="right">4.09</td>
        </tr>
        <tr>
          <td align="left">Machines</td>
          <td align="right">1</td>
          <td align="right">3.83</td>
          <td align="right">11</td>
          <td align="right">2.17</td>
          <td align="right">3.43</td>
          <td align="right">4.23</td>
        </tr>
        <tr>
          <td align="left">Paper</td>
          <td align="right">1</td>
          <td align="right">3.78</td>
          <td align="right">14</td>
          <td align="right">2.23</td>
          <td align="right">3.66</td>
          <td align="right">3.90</td>
        </tr>
        <tr>
          <td align="left">Phones</td>
          <td align="right">1</td>
          <td align="right">3.70</td>
          <td align="right">14</td>
          <td align="right">2.19</td>
          <td align="right">3.56</td>
          <td align="right">3.84</td>
        </tr>
        <tr>
          <td align="left">Storage</td>
          <td align="right">1</td>
          <td align="right">3.73</td>
          <td align="right">14</td>
          <td align="right">2.19</td>
          <td align="right">3.58</td>
          <td align="right">3.88</td>
        </tr>
        <tr>
          <td align="left">Supplies</td>
          <td align="right">1</td>
          <td align="right">3.41</td>
          <td align="right">10</td>
          <td align="right">1.84</td>
          <td align="right">3.15</td>
          <td align="right">3.67</td>
        </tr>
        <tr>
          <td align="left">Tables</td>
          <td align="right">1</td>
          <td align="right">3.89</td>
          <td align="right">13</td>
          <td align="right">2.45</td>
          <td align="right">3.62</td>
          <td align="right">4.16</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <p>The statistics for Quantity aggregated by Sub_Category looks valid.
  We can visualize it as histogram and check for anomalies. Figure y
  contains histograms of Quantity per Sub_Category.</p>
  <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/sub_category_histograms-1.png" />
  <p>The histograms show that the quantities a right-skewed distributed.
  This is to be expected since most orders contain only a small number
  of items. We will not remove the outliers with large quantities since
  they appear valid..</p>
</sec>
<sec id="data-visualization">
  <title>3 Data Visualization</title>
</sec>
<sec id="forecasting-method-evaluation">
  <title>4 Forecasting Method Evaluation</title>
  <sec id="forecasting-top-3-product-categories-4a">
    <title>Forecasting top 3 product categories (4a)</title>
    <p>Let’s forecast sold quantities for the three most sold
    sub-categories:</p>
    <p>The steps taken for data preparation were:</p>
    <list list-type="bullet">
      <list-item>
        <p>Identifying Top Subcategories: The top three subcategories
        are selected from our dataset based on their sold quantities.
        The top three were: Binders, furnishing and paper.</p>
      </list-item>
      <list-item>
        <p>The sold quantities are aggregated monthly to create a time
        series object which we can use in the forecasting.</p>
      </list-item>
      <list-item>
        <p>A KPSS showed that the data is non stationary. First-order
        differencing is applied to transform the data from
        non-stationary to stationary. The KPSS results in a p-value
        &gt;0.05 showing the stationarity.</p>
      </list-item>
    </list>
    <p>Three models are applied to each subcategory to forecast it. The
    models we use are: ARIMA, Holt-Winters and ETS. We have chosen these
    models because of their level of suitability for discrete time
    series data with all different levels of trend and seasonality. To
    evaluate the methods and its effectiveness , the data is split into
    a training set (70%) and testing set (30%).</p>
    <p>To assess the results, we use the following performance metrics:
    ME, RMSE, MAE and MAPE. They are calculated for the training and
    testing phases of the forecast.</p>
    <p>As we can see on the forecasting results ARIMA performed well for
    binders. We can state this because of the lowest RMSE if you compare
    it to the other models.</p>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/Results1-1.png" />
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/Results1-2.png" />
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/Results1-3.png" />
    <preformat>
    KPSS Test for Level Stationarity

data:  ts_diff
KPSS Level = 0.10182, Truncation lag parameter = 3, p-value = 0.1


    KPSS Test for Level Stationarity

data:  ts_diff
KPSS Level = 0.061982, Truncation lag parameter = 3, p-value = 0.1


    KPSS Test for Level Stationarity

data:  ts_diff
KPSS Level = 0.098438, Truncation lag parameter = 3, p-value = 0.1</preformat>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/results3-1.png" />
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/Results4-1.png" />
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/Results4-2.png" />
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/Results4-3.png" />
    <preformat>
KPSS Test for Differenced Sub-Category: Binders 

    KPSS Test for Level Stationarity

data:  ts_current
KPSS Level = 0.10182, Truncation lag parameter = 3, p-value = 0.1


KPSS Test for Differenced Sub-Category: Paper 

    KPSS Test for Level Stationarity

data:  ts_current
KPSS Level = 0.061982, Truncation lag parameter = 3, p-value = 0.1


KPSS Test for Differenced Sub-Category: Furnishings 

    KPSS Test for Level Stationarity

data:  ts_current
KPSS Level = 0.098438, Truncation lag parameter = 3, p-value = 0.1</preformat>
    <p>As we can see on the forecasting results ARIMA performed well for
    binders. We can state this because of the lowest RMSE. - ARIMA
    Binders: - Forecasting results Binders:</p>
    <p>For the subcategory furnishings we can see that the ETS
    forecasting method is the most stable across the training and
    testing phase. - ETS furnishings - Forecasting results
    Furnishings:</p>
    <p>For the last subcategory and product paper the ETS model is again
    the most consistent, comparing the statistics for training and test
    set. The high variability in the test data leads to larger
    forecasting errors in all the 3 models.</p>
    <list list-type="bullet">
      <list-item>
        <p>ETS Furnishings:</p>
      </list-item>
      <list-item>
        <p>Forecasting results:</p>
      </list-item>
    </list>
    <p>Residual Diagnostics: - The checks show no real autocorrelation
    for ARIMA models. Which indicates a good fitting forecast.</p>
  </sec>
  <sec id="conclusion-4a">
    <title>Conclusion (4a)</title>
    <p>The most effective model is not the same in all the
    subcategories. Each model was validated based on its ability to
    capture seasonality and trend. ARIMA performed better for Binders,
    while ETS performed better for Furnishings and Paper.</p>
  </sec>
  <sec id="clustering-4b">
    <title>Clustering (4b)</title>
    <preformat>

Results for Cluster_1 

Sub-Category: Binders 

ARIMA Accuracy:
                    ME      RMSE      MAE        MPE     MAPE      MASE
Training set 0.7706014  4.643476 2.982256  0.6865304 12.91204 0.4854835
Test set     5.9407398 10.783528 7.616473 10.3681817 17.32927 1.2398909
                   ACF1 Theil's U
Training set 0.04429472        NA
Test set     0.04929320 0.3573866

Holt-Winters Accuracy:
                    ME     RMSE      MAE        MPE     MAPE      MASE
Training set 0.8668058 5.215491 3.789990 -0.4040374 15.36545 0.6169751
Test set     2.2473496 8.712049 6.243226  0.1597635 16.02219 1.0163391
                     ACF1 Theil's U
Training set -0.389045966        NA
Test set     -0.001830843 0.2777843

ETS Accuracy:
                   ME      RMSE      MAE       MPE     MAPE      MASE
Training set 0.743354  4.712561 3.656409  1.358692 15.41708 0.5952293
Test set     7.439784 12.216161 8.825094 10.667088 20.93561 1.4366433
                   ACF1 Theil's U
Training set -0.2225537        NA
Test set      0.0608554 0.3767484


Results for Cluster_2 

Sub-Category: Paper 

ARIMA Accuracy:
                    ME      RMSE       MAE        MPE     MAPE      MASE
Training set  1.117384  6.309464  3.827714   1.702165 14.32157 0.5468163
Test set     -9.014128 12.230774 10.375521 -30.103886 31.89519 1.4822173
                     ACF1 Theil's U
Training set -0.007064333        NA
Test set      0.108516273 0.6984566

Holt-Winters Accuracy:
                     ME     RMSE       MAE        MPE     MAPE      MASE
Training set   1.509544  7.27986  4.854547   3.325281 16.41309 0.6935067
Test set     -12.036865 14.48061 13.347292 -39.791842 41.51609 1.9067560
                    ACF1 Theil's U
Training set -0.02710216        NA
Test set      0.10006713  0.845232

ETS Accuracy:
                     ME      RMSE      MAE         MPE     MAPE      MASE
Training set 0.60941117  6.204628 4.474990 -1.66197953 18.73240 0.6392842
Test set     0.08500424 11.304488 8.247017 -0.08462612 20.47598 1.1781453
                    ACF1 Theil's U
Training set 0.005205508        NA
Test set     0.341519997  0.582549


Results for Cluster_3 

Sub-Category: Furnishings 

ARIMA Accuracy:
                    ME     RMSE      MAE       MPE     MAPE      MASE
Training set 0.0050974 3.873555 2.567868 -6.624673 20.20958 0.5559302
Test set     3.9523810 7.782677 6.238095 10.720079 22.75340 1.3505155
                    ACF1 Theil's U
Training set -0.20160077        NA
Test set     -0.03570035 0.6102339

Holt-Winters Accuracy:
                    ME     RMSE      MAE      MPE     MAPE      MASE
Training set 0.9137655 4.164677 3.475419 7.490317 24.75583 0.7524103
Test set     3.6371987 7.200985 5.673333 9.724318 20.50134 1.2282474
                    ACF1 Theil's U
Training set -0.43317305        NA
Test set      0.01804785 0.5689788

ETS Accuracy:
                    ME     RMSE      MAE        MPE     MAPE     MASE
Training set 0.7370579 3.466690 2.851745  0.4301648 20.85220 0.617388
Test set     6.0973038 8.354163 6.690832 20.7276401 23.55317 1.448531
                   ACF1 Theil's U
Training set -0.2087083        NA
Test set      0.3729315 0.7455977</preformat>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/somelable-1.png" />
    <preformat>
Residual Diagnostics for Sub-Category: Binders </preformat>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/somelable-2.png" />
    <preformat>
    Ljung-Box test

data:  Residuals from ARIMA(0,1,2)(0,1,0)[12]
Q* = 2.6295, df = 5, p-value = 0.7569

Model df: 2.   Total lags used: 7


Residual Diagnostics for Sub-Category: Paper </preformat>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/somelable-3.png" />
    <preformat>
    Ljung-Box test

data:  Residuals from ARIMA(0,1,1)(0,1,0)[12]
Q* = 6.6676, df = 6, p-value = 0.3527

Model df: 1.   Total lags used: 7


Residual Diagnostics for Sub-Category: Furnishings </preformat>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/somelable-4.png" />
    <preformat>
    Ljung-Box test

data:  Residuals from ARIMA(0,0,0)(0,1,0)[12] with drift
Q* = 9.5952, df = 7, p-value = 0.2127

Model df: 0.   Total lags used: 7</preformat>
    <preformat>    Cluster  MeanRMSE MeanMAPE
1 Cluster_1 10.783528 17.32927
2 Cluster_2 12.230774 31.89519
3 Cluster_3  7.782677 22.75340</preformat>
    <p>Cluster 1 (e.g., Binders): ARIMA outperformed other methods due
    to significant autocorrelation and trend components.</p>
    <p>Cluster 2 (e.g., Furnishings): ETS was the most accurate method,
    effectively balancing trend and seasonality.</p>
    <p>Cluster 3 (e.g., Paper): ETS also performed best, with ARIMA
    showing higher error rates due to variability in random
    components.</p>
    <p>Residual diagnostics were performed for all ARIMA models,
    confirming no significant autocorrelation (p &gt; 0.05).</p>
    <p>Cluster-Level Metrics based on mean RMSE and MAPE show: - Cluster
    1 had the lowest RMSE using ARIMA. - Cluster 2 and 3 were better
    modeled with ETS</p>
  </sec>
  <sec id="conclusion-4b">
    <title>Conclusion (4b)</title>
    <p>Clustering allows for tailored forecasting strategies. We
    conclude that for the given data set ARIMA is more effective for
    clusters with strong trends, while ETS is preferable for clusters
    with mixed seasonal and trend characteristics. The approach aligns
    with lecture notes, emphasizing the importance of adapting models
    based on time series characteristics.</p>
  </sec>
</sec>
<sec id="forecasting-future-values">
  <title>5 Forecasting future values</title>
  <sec id="forecasting-3-products-5a">
    <title>Forecasting 3 products (5a)</title>
    <p>In this session, we focused on evaluating different forecasting
    models (ARIMA, Holt-Winters, and ETS) for multiple sub-categories by
    analyzing their accuracy metrics, such as RMSE, MAPE, and residual
    diagnostics. Based on the evaluation results, we selected the
    best-performing model for each sub-category. We then used these
    models to forecast the future outcomes for each sub-category,
    projecting the data for the next year. Note: we may need to
    interpret the outcomes and explain why we pick the certain model</p>
    <preformat>Series: binders_ts 
ARIMA(1,1,1)(0,1,0)[12] 

Coefficients:
          ar1      ma1
      -0.4781  -0.4819
s.e.   0.2324   0.2426

sigma^2 = 51.18:  log likelihood = -117.97
AIC=241.94   AICc=242.72   BIC=246.61

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE     MASE        ACF1
Training set 0.864453 5.931761 4.092168 -1.363101 15.06142 0.558023 -0.03746012</preformat>
    <preformat>         Point Forecast    Lo 80     Hi 80     Lo 95     Hi 95
Jan 2018       32.48390 23.31571  41.65208 18.462370  46.50543
Feb 2018       23.77181 14.59632  32.94731  9.739103  37.80452
Mar 2018       45.85265 35.59989  56.10541 30.172412  61.53289
Apr 2018       46.20475 35.63662  56.77287 30.042185  62.36730
May 2018       44.08009 32.93968  55.22051 27.042295  61.11789
Jun 2018       44.61784 33.06359  56.17210 26.947139  62.28855
Jul 2018       40.36072 28.34867  52.37277 21.989869  58.73157
Aug 2018       44.48366 32.05796  56.90937 25.480189  63.48714
Sep 2018       73.42488 60.58630  86.26346 53.789962  93.05979
Oct 2018       51.45299 38.22024  64.68573 31.215253  71.69072
Nov 2018       72.43955 58.82134  86.05775 51.612293  93.26680
Dec 2018       89.44597 75.45417 103.43777 68.047363 110.84458</preformat>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-18-1.png" />
    <preformat>ETS(M,N,A) 

Call:
ets(y = paper_ts)

  Smoothing parameters:
    alpha = 0.3075 
    gamma = 1e-04 

  Initial states:
    l = 22.5954 
    s = 17.4472 16.5763 -4.1253 15.2986 0.421 -5.102
           -0.6145 -0.0341 -7.985 -2.6766 -15.6576 -13.5481

  sigma:  0.2365

     AIC     AICc      BIC 
365.1517 380.1517 393.2197 

Training set error measures:
                   ME     RMSE      MAE     MPE     MAPE      MASE       ACF1
Training set 1.450303 6.386648 4.166875 1.75373 14.03399 0.5245018 0.03600045</preformat>
    <preformat>         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95
Jan 2018       30.45588 21.22661 39.68516 16.34092 44.57085
Feb 2018       28.34651 19.27484 37.41819 14.47258 42.22044
Mar 2018       41.32776 28.18367 54.47185 21.22561 61.42990
Apr 2018       36.01933 23.73891 48.29976 17.23804 54.80062
May 2018       43.97017 29.09477 58.84557 21.22021 66.72013
Jun 2018       43.39031 28.07408 58.70653 19.96616 66.81445
Jul 2018       38.90220 24.12853 53.67588 16.30782 61.49659
Aug 2018       44.42410 27.84663 61.00158 19.07104 69.77717
Sep 2018       59.30522 38.44478 80.16566 27.40193 91.20850
Oct 2018       39.87917 22.81850 56.93984 13.78712 65.97122
Nov 2018       60.58102 38.27853 82.88352 26.47230 94.68975
Dec 2018       61.45110 38.17748 84.72473 25.85717 97.04504</preformat>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-18-2.png" />
    <preformat>ETS(M,A,A) 

Call:
ets(y = furnishings_ts)

  Smoothing parameters:
    alpha = 0.0438 
    beta  = 0.0437 
    gamma = 2e-04 

  Initial states:
    l = 15.4275 
    b = -0.1137 
    s = 13.3158 15.6269 -2.2962 10.1503 -5.0017 -2.448
           -3.4406 -1.0728 -1.1262 -4.3688 -11.689 -7.6497

  sigma:  0.2527

     AIC     AICc      BIC 
338.8888 359.2888 370.6992 

Training set error measures:
                    ME     RMSE      MAE        MPE    MAPE      MASE
Training set 0.6402485 3.793384 2.884208 -0.8302416 16.2441 0.5352139
                   ACF1
Training set 0.04613441</preformat>
    <preformat>         Point Forecast    Lo 80    Hi 80     Lo 95    Hi 95
Jan 2018       21.37433 14.45350 28.29515 10.789837 31.95882
Feb 2018       18.56644 12.52244 24.61044  9.322944 27.80994
Mar 2018       27.11574 18.26943 35.96205 13.586481 40.64500
Apr 2018       31.58782 21.22159 41.95404 15.734048 47.44158
May 2018       32.87189 21.95559 43.78819 16.176845 49.56693
Jun 2018       31.73384 20.95052 42.51717 15.242170 48.22551
Jul 2018       33.95710 22.18459 45.72960 15.952604 51.96159
Aug 2018       32.63192 20.83816 44.42569 14.594916 50.66893
Sep 2018       49.01546 31.92061 66.11032 22.871140 75.15979
Oct 2018       37.79884 23.38308 52.21460 15.751844 59.84584
Nov 2018       56.95159 36.44698 77.45621 25.592490 88.31070
Dec 2018       55.87232 34.96477 76.77986 23.896983 87.84765</preformat>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-18-3.png" />
  </sec>
  <sec id="applying-to-all-data-5b">
    <title>Applying to all data (5b)</title>
    <p>In this session, we first grouped the sub-categories into
    clusters based on key time-series features, including trend
    strength, seasonal strength, and random strength, using hierarchical
    clustering. Once the clusters were formed, we applied and evaluated
    multiple forecasting models—ARIMA, Holt-Winters, and ETS—on each
    sub-category within its respective cluster, comparing their accuracy
    metrics such as RMSE and MAPE. Based on the evaluation results, we
    selected the best-performing model for each sub-category and used it
    to forecast the future outcomes within a year, leveraging the
    clustering to enhance the accuracy and relevance of our
    predictions.</p>
  </sec>
</sec>
<sec id="forecast-interpretation">
  <title>6 Forecast interpretation</title>
  <p>Lorem Ipsum</p>
  <p></p>
  <sec id="forecasting">
    <title>Forecasting??</title>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-21-1.png" />
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-21-2.png" />
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-21-3.png" />
  </sec>
</sec>
</body>

<back>
</back>

<sub-article article-type="notebook" id="nb-7-nb-article">
<front-stub>
<title-group>
<article-title>Supply Chain Data Analytics</article-title>
<subtitle>Analyzing and Forcasting Supermarket Sales</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>(2671939)</surname>
<given-names>Stan Brouwer</given-names>
</name>
<string-name>Stan Brouwer (2671939)</string-name>

<xref ref-type="aff" rid="aff-1-nb-article">a</xref>
<xref ref-type="corresp" rid="cor-1-nb-article">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>(2840693)</surname>
<given-names>Liz Chen</given-names>
</name>
<string-name>Liz Chen (2840693)</string-name>

<xref ref-type="aff" rid="aff-2-nb-article">b</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>(2854979)</surname>
<given-names>Maaike Lamberts</given-names>
</name>
<string-name>Maaike Lamberts (2854979)</string-name>

<xref ref-type="aff" rid="aff-3-nb-article">c</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Schroor</surname>
<given-names>Niek</given-names>
</name>
<string-name>Niek Schroor</string-name>

<xref ref-type="aff" rid="aff-4-nb-article">d</xref>
</contrib>
</contrib-group>
<aff id="aff-1-nb-article">
<institution-wrap>
<institution>Vrije Universiteit</institution>
</institution-wrap>







</aff>
<aff id="aff-2-nb-article">
<institution-wrap>
<institution>Master TSCM</institution>
</institution-wrap>







</aff>
<aff id="aff-3-nb-article">
<institution-wrap>
<institution>Supply Chain Data analysis</institution>
</institution-wrap>







</aff>
<aff id="aff-4-nb-article">
<institution-wrap>
<institution>Group 10</institution>
</institution-wrap>







</aff>
<author-notes>
<corresp id="cor-1-nb-article"></corresp>
</author-notes>
</front-stub>

<body>
<sec id="data-selection-nb-article">
  <title>1 Data selection</title>
  <p>We analyze, forecast and interpret the
  <ext-link ext-link-type="uri" xlink:href="https://public.tableau.com/app/sample-data/sample_-_superstore.xls">Superstore
  sales</ext-link> provided by
  <ext-link ext-link-type="uri" xlink:href="https://public.tableau.com/app/learn/sample-data">Tableau</ext-link>
  using different statistical and machine learning methods.</p>
  <p>The dataset provided contains information about products, sales and
  profits of a fictitious US company. The dataset contains about 10,000
  rows with 1,850 unique product names and 17 product subcategories,
  covering four consecutive years on a daily basis.</p>
  <p>We describe our work in the PDF version. However, we would like to
  recommend reading our quarto manuscript <italic>here</italic> as it
  contains the <bold>relevant</bold> R code in the Article Notebook.</p>
</sec>
<sec id="data-pre-processing-nb-article">
  <title>2 Data Pre-processing</title>
  <p>The superstore data set we selected is of high quality: At first
  glance (which needs to be verified during the visualization), the data
  appears to have been recorded regularly and without interruptions.
  There is no sign of a sudden structural change. Since the data are
  consumer products, it should contain both trends and seasonality.
  Nevertheless, we have included hypothetical steps to demonstrate our
  understanding of the data preprocessing procedure. In detail, we
  did:</p>
  <sec specific-use="notebook-content">
  <code language="r script"># Clear workspace
rm(list = ls())
# Function to load (and install if necessary) dependencies
install_and_load &lt;- function(packages) {
  install.packages(setdiff(packages, rownames(installed.packages())), dependencies = TRUE)
  invisible(lapply(packages, require, character.only = TRUE))
}
install_and_load(c(&quot;tidyverse&quot;, &quot;readxl&quot;, &quot;ggplot2&quot;, &quot;lubridate&quot;, &quot;stats&quot;, &quot;Amelia&quot;,&quot;forecast&quot;, &quot;tseries&quot;, &quot;plotly&quot;, &quot;stringr&quot;, &quot;knitr&quot;, &quot;kableExtra&quot;))</code>
  <boxed-text>
    <preformat>Loading required package: tidyverse</preformat>
  </boxed-text>
  <boxed-text>
    <preformat>── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ ggplot2   3.5.1     ✔ tibble    3.2.1
✔ lubridate 1.9.3     ✔ tidyr     1.3.1
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors
Loading required package: readxl

Loading required package: Amelia</preformat>
  </boxed-text>
  <boxed-text>
    <preformat>Warning: package 'Amelia' was built under R version 4.3.3</preformat>
  </boxed-text>
  <boxed-text>
    <preformat>Loading required package: Rcpp
## 
## Amelia II: Multiple Imputation
## (Version 1.8.3, built: 2024-11-07)
## Copyright (C) 2005-2024 James Honaker, Gary King and Matthew Blackwell
## Refer to http://gking.harvard.edu/amelia/ for more information
## 
Loading required package: forecast</preformat>
  </boxed-text>
  <boxed-text>
    <preformat>Warning: package 'forecast' was built under R version 4.3.3</preformat>
  </boxed-text>
  <boxed-text>
    <preformat>Registered S3 method overwritten by 'quantmod':
  method            from
  as.zoo.data.frame zoo 
Loading required package: tseries</preformat>
  </boxed-text>
  <boxed-text>
    <preformat>Warning: package 'tseries' was built under R version 4.3.3</preformat>
  </boxed-text>
  <boxed-text>
    <preformat>Loading required package: plotly

Attaching package: 'plotly'

The following object is masked from 'package:ggplot2':

    last_plot

The following object is masked from 'package:stats':

    filter

The following object is masked from 'package:graphics':

    layout

Loading required package: knitr</preformat>
  </boxed-text>
  <boxed-text>
    <preformat>Warning: package 'knitr' was built under R version 4.3.3</preformat>
  </boxed-text>
  <boxed-text>
    <preformat>Loading required package: kableExtra

Attaching package: 'kableExtra'

The following object is masked from 'package:dplyr':

    group_rows</preformat>
  </boxed-text>
  </sec>
  <list list-type="bullet">
    <list-item>
      <p>Remove whitespaces from column names</p>
    </list-item>
    <list-item>
      <p>Remove the Row_ID column as it can be inferred by it’s
      index</p>
    </list-item>
    <list-item>
      <p>Remove all columns with a single unique value, as storing these
      would be
      <ext-link ext-link-type="uri" xlink:href="https://few.vu.nl/~molenaar/courses/StatR/chapters/B-06-raw_data.html">redundant</ext-link></p>
    </list-item>
    <list-item>
      <p>Ensure machine-readable date formats in yyyy-mm-dd as these
      usually differ per locale.</p>
    </list-item>
    <list-item>
      <p>Ensure proper decimal separators</p>
    </list-item>
    <list-item>
      <p>Calculate the number of missing values (both NA and empty
      string ““) per column.</p>
    </list-item>
  </list>
  <sec specific-use="notebook-content">
  <code language="r script"># Load the data
suppressWarnings({data &lt;- read_excel(&quot;data/sample_-_superstore.xls&quot;)}) # The Postal code column is stored as 'text' but coerced to numeric, causing warnings which we suppress

# Improve column names
colnames(data) &lt;- str_replace_all(colnames(data), &quot; &quot;, &quot;_&quot;)
colnames(data) &lt;- str_replace_all(colnames(data), &quot;-&quot;, &quot;_&quot;)

# Remove the 'Row_ID' column as it can be inferred by it's index
data &lt;- subset(data, select = -`Row_ID`)

# Remove all columns that have only one unique value, as storing these would be redundant
data &lt;- data[, sapply(data, function(col) length(unique(col)) &gt; 1)]

# Ensure a machine-readable date format as these are usually horrible in excel files
data$Order_Date &lt;- as.Date(data$Order_Date, format = &quot;%Y-%m-%d&quot;)
data$Ship_Date &lt;- as.Date(data$Ship_Date, format = &quot;%Y-%m-%d&quot;)

# The readxl package by default uses the correct decimal separator (as opposed to base R)

# Calculate the number of missing values per column.
# Origional dates and R date objects are in unix time, which return NA when compared to text (empty string). These dates are stored as 'double' datatype, Thus we check character columns for empty strings, and all columns for NA values. 
missing_values &lt;- sapply(data, function(col) {
  if (inherits(col, &quot;Date&quot;)) {
    sum(is.na(col))
  } else if (is.character(col)) {
    sum(is.na(col) | col == &quot;&quot;)
  } else {
    sum(is.na(col))
  }
})

# sum(missing_values) returns 0!

# Optionally, print the missing values as a nice table
missing_values_table &lt;- data.frame(
  Column = names(missing_values),
  Missing_or_Empty = missing_values
)
# Note that there are no missing values, thus we do not print them
# kable(missing_values_table, caption = &quot;Missing or Empty Values in Columns&quot;, format = &quot;pipe&quot;)


rm(missing_values, missing_values_table)</code>
  </sec>
  <p>After these steps (and transposing the table for better document
  formatting), the data looks as follows:</p>
  <sec specific-use="notebook-content">
  <code language="r script">kable(t(head(data, 3)), caption = &quot;First 3 Rows of the Data (Transposed)&quot;, format = &quot;markdown&quot;)</code>
  <table-wrap>
    <caption>
      <p>First 3 Rows of the Data (Transposed)</p>
    </caption>
    <table>
      <colgroup>
        <col width="8%" />
        <col width="20%" />
        <col width="36%" />
        <col width="35%" />
      </colgroup>
      <tbody>
        <tr>
          <td align="left">Order_ID</td>
          <td align="left">CA-2016-152156</td>
          <td align="left">CA-2016-152156</td>
          <td align="left">CA-2016-138688</td>
        </tr>
        <tr>
          <td align="left">Order_Date</td>
          <td align="left">2016-11-08</td>
          <td align="left">2016-11-08</td>
          <td align="left">2016-06-12</td>
        </tr>
        <tr>
          <td align="left">Ship_Date</td>
          <td align="left">2016-11-11</td>
          <td align="left">2016-11-11</td>
          <td align="left">2016-06-16</td>
        </tr>
        <tr>
          <td align="left">Ship_Mode</td>
          <td align="left">Second Class</td>
          <td align="left">Second Class</td>
          <td align="left">Second Class</td>
        </tr>
        <tr>
          <td align="left">Customer_ID</td>
          <td align="left">CG-12520</td>
          <td align="left">CG-12520</td>
          <td align="left">DV-13045</td>
        </tr>
        <tr>
          <td align="left">Customer_Name</td>
          <td align="left">Claire Gute</td>
          <td align="left">Claire Gute</td>
          <td align="left">Darrin Van Huff</td>
        </tr>
        <tr>
          <td align="left">Segment</td>
          <td align="left">Consumer</td>
          <td align="left">Consumer</td>
          <td align="left">Corporate</td>
        </tr>
        <tr>
          <td align="left">City</td>
          <td align="left">Henderson</td>
          <td align="left">Henderson</td>
          <td align="left">Los Angeles</td>
        </tr>
        <tr>
          <td align="left">State</td>
          <td align="left">Kentucky</td>
          <td align="left">Kentucky</td>
          <td align="left">California</td>
        </tr>
        <tr>
          <td align="left">Postal_Code</td>
          <td align="left">42420</td>
          <td align="left">42420</td>
          <td align="left">90036</td>
        </tr>
        <tr>
          <td align="left">Region</td>
          <td align="left">South</td>
          <td align="left">South</td>
          <td align="left">West</td>
        </tr>
        <tr>
          <td align="left">Product_ID</td>
          <td align="left">FUR-BO-10001798</td>
          <td align="left">FUR-CH-10000454</td>
          <td align="left">OFF-LA-10000240</td>
        </tr>
        <tr>
          <td align="left">Category</td>
          <td align="left">Furniture</td>
          <td align="left">Furniture</td>
          <td align="left">Office Supplies</td>
        </tr>
        <tr>
          <td align="left">Sub_Category</td>
          <td align="left">Bookcases</td>
          <td align="left">Chairs</td>
          <td align="left">Labels</td>
        </tr>
        <tr>
          <td align="left">Product_Name</td>
          <td align="left">Bush Somerset Collection Bookcase</td>
          <td align="left">Hon Deluxe Fabric Upholstered Stacking
          Chairs, Rounded Back</td>
          <td align="left">Self-Adhesive Address Labels for Typewriters
          by Universal</td>
        </tr>
        <tr>
          <td align="left">Sales</td>
          <td align="left">261.96</td>
          <td align="left">731.94</td>
          <td align="left">14.62</td>
        </tr>
        <tr>
          <td align="left">Quantity</td>
          <td align="left">2</td>
          <td align="left">3</td>
          <td align="left">2</td>
        </tr>
        <tr>
          <td align="left">Discount</td>
          <td align="left">0</td>
          <td align="left">0</td>
          <td align="left">0</td>
        </tr>
        <tr>
          <td align="left">Profit</td>
          <td align="left">41.9136</td>
          <td align="left">219.5820</td>
          <td align="left">6.8714</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  </sec>
  <p>We did not find any missing values, confirming the quality of the
  data set. There is some more processing to do, for instance the
  removal of outliers. However, by doing so we impose our own
  assumptions on the data. Let’s start by evaluating the descriptive
  statistics of our data and check if further processing is
  required.</p>
  <sec specific-use="notebook-content">
  <code language="r script">descriptive_statistics &lt;- function(column) {
  if (is.numeric(column)) {
    stats &lt;- list(
      Min = min(column, na.rm = TRUE), # Note that handling NA values increases robustness (and I copied the funciton from some of my earlier work)
      Max = max(column, na.rm = TRUE),
      Mean = mean(column, na.rm = TRUE),
      Median = median(column, na.rm = TRUE),
      StdDev = sd(column, na.rm = TRUE)
    )
  } else if (inherits(column, &quot;Date&quot;)) {
    stats &lt;- list(
      Earliest = format(min(column, na.rm = TRUE), &quot;%Y-%m-%d&quot;),
      Latest = format(max(column, na.rm = TRUE), &quot;%Y-%m-%d&quot;)
    )
  } else if (is.character(column)) {
    stats &lt;- list(
      Unique = length(unique(column)),
      Mode = names(sort(table(column), decreasing = TRUE)[1])
    )
  } else {
    stats &lt;- NULL
  }
  return(stats)
}

# Call function on dataframe
descriptive_stats &lt;- lapply(data, descriptive_statistics)

# Separate to tables dependent on data type
numeric_stats &lt;- as.data.frame(do.call(rbind, lapply(names(data), function(col_name) {
  if (is.numeric(data[[col_name]])) {
    c(Column = col_name, descriptive_stats[[col_name]])
  }
})), stringsAsFactors = FALSE)
date_stats &lt;- as.data.frame(do.call(rbind, lapply(names(data), function(col_name) {
  if (inherits(data[[col_name]], &quot;Date&quot;)) {
    c(Column = col_name, descriptive_stats[[col_name]])
  }
})), stringsAsFactors = FALSE)
character_stats &lt;- as.data.frame(do.call(rbind, lapply(names(data), function(col_name) {
  if (is.character(data[[col_name]])) {
    c(Column = col_name, descriptive_stats[[col_name]])
  }
})), stringsAsFactors = FALSE)</code>
  </sec>
  <sec specific-use="notebook-content">
  <code language="r script">kable(
  numeric_stats,
  caption = &quot;Descriptive Statistics for Numeric Columns&quot;,
  format = &quot;pipe&quot;)</code>
  <table-wrap>
    <caption>
      <p>Descriptive Statistics for Numeric Columns</p>
    </caption>
    <table>
      <thead>
        <tr>
          <th align="left">Column</th>
          <th align="left">Min</th>
          <th align="left">Max</th>
          <th align="left">Mean</th>
          <th align="left">Median</th>
          <th align="left">StdDev</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left">Postal_Code</td>
          <td align="left">1040</td>
          <td align="left">99301</td>
          <td align="left">55190.38</td>
          <td align="left">56430.5</td>
          <td align="left">32063.69</td>
        </tr>
        <tr>
          <td align="left">Sales</td>
          <td align="left">0.444</td>
          <td align="left">22638.48</td>
          <td align="left">229.858</td>
          <td align="left">54.49</td>
          <td align="left">623.2451</td>
        </tr>
        <tr>
          <td align="left">Quantity</td>
          <td align="left">1</td>
          <td align="left">14</td>
          <td align="left">3.789574</td>
          <td align="left">3</td>
          <td align="left">2.22511</td>
        </tr>
        <tr>
          <td align="left">Discount</td>
          <td align="left">0</td>
          <td align="left">0.8</td>
          <td align="left">0.1562027</td>
          <td align="left">0.2</td>
          <td align="left">0.206452</td>
        </tr>
        <tr>
          <td align="left">Profit</td>
          <td align="left">-6599.978</td>
          <td align="left">8399.976</td>
          <td align="left">28.6569</td>
          <td align="left">8.6665</td>
          <td align="left">234.2601</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <code language="r script">kable(
  date_stats,
  caption = &quot;Descriptive Statistics for Date Columns&quot;,
  format = &quot;pipe&quot;)</code>
  <table-wrap>
    <caption>
      <p>Descriptive Statistics for Date Columns</p>
    </caption>
    <table>
      <thead>
        <tr>
          <th align="left">Column</th>
          <th align="left">Earliest</th>
          <th align="left">Latest</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left">Order_Date</td>
          <td align="left">2014-01-03</td>
          <td align="left">2017-12-30</td>
        </tr>
        <tr>
          <td align="left">Ship_Date</td>
          <td align="left">2014-01-07</td>
          <td align="left">2018-01-05</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  </sec>
  <p>We inspect the orders with the lowest and highest Sales amount (in
  USD). The most expensive orders were professional printers, cameras
  and teleconferencing units with high unit prices. The orders with the
  lowest sales amount were often binders and had a high Discount
  rate.</p>
  <p>Interestingly there are orders with a negative profit. They
  typically have high Discount rates and often concern the same item,
  such as the “Cubify CubeX 3D Printer Triple Head Print”. The orders
  with a negative Profit were often part of a larger order (for instance
  CA-2016-108196), and placed by customers with multiple orders. We
  suspect these negative Profit’s to be caused by items of lower quality
  that receive discounts, general discount codes, or volume discounts.
  However, due to the high discounts especially on orders with negative
  profit, we assume these to be valid orders.</p>
  <p>** Some negative profit products **</p>
  <p>In figure x we plotted the quantities of the most sold products.
  Unfortunately, the sold quantities of individual products were too low
  to determine any meaningful trends.</p>
  <sec specific-use="notebook-content">
  <code language="r script"># Optionally: print top 10 sale quantity barplot
# # Sum of Quantity for top products
# top_products &lt;- data %&gt;%
#   group_by(Product_Name) %&gt;%
#   summarize(total_quantity = sum(Quantity, na.rm = TRUE)) %&gt;%
#   arrange(desc(total_quantity)) %&gt;%
#   slice_head(n = 10) %&gt;% 
#   mutate(ProdName8 = substr(Product_Name, 1, 8)) # Truncate product names to the first 8 characters. Long names mess up formatting
# 
# # Plot
# ggplot(top_products, aes(x = reorder(ProdName8, -total_quantity), y = total_quantity)) +
#   geom_bar(stat = &quot;identity&quot;, fill = &quot;steelblue&quot;) +
#   labs(title = &quot;Top 20 Most Sold Products&quot;,
#        x = &quot;Product ID&quot;,
#        y = &quot;Total Quantity&quot;) +
#   theme_minimal() +
#   coord_flip()

# Aggregate quantity by Product Name and Order Date to create a time series
time_series_data &lt;- data %&gt;%
  group_by(Product_Name, Order_Date) %&gt;%
  summarize(total_quantity = sum(Quantity, na.rm = TRUE)) %&gt;%
  ungroup()</code>
  <boxed-text>
    <preformat>`summarise()` has grouped output by 'Product_Name'. You can override using the
`.groups` argument.</preformat>
  </boxed-text>
  <code language="r script"># Filter for the top products by total quantity sold (adjust as needed)
top_products &lt;- time_series_data %&gt;%
  group_by(Product_Name) %&gt;%
  summarize(total_quantity = sum(total_quantity)) %&gt;%
  arrange(desc(total_quantity)) %&gt;%
  slice_head(n = 10)  # Select top 10 products

# Filter the time-series data for only these top products
filtered_time_series_data &lt;- time_series_data %&gt;%
  filter(Product_Name %in% top_products$Product_Name) %&gt;%
  mutate(ProdName10 = substr(Product_Name, 1, 10)) # Product names can be quite long and mess up layouts. Lets only plot the first 10 chars.

# Here we do some special plotting. We want to show the plot with only one selected line by default, but make sure that the other 9 top sold products can be selected. We first create the ggplotly object, and than modify the visibility of the traces</code>
  </sec>
  <sec specific-use="notebook-content">
  <code language="r script"># Plot interactive figure in html, plot ggplot2 in pdf:

# Creating the ggplotly object
p_ly &lt;- ggplotly(ggplot(filtered_time_series_data, aes(x = Order_Date, y = total_quantity, color = ProdName10)) +
  geom_line(size = 1) +
  labs(title = &quot;Quantity Sold Over Time per Product&quot;,
       x = &quot;Order Date&quot;,
       y = &quot;Quantity Sold&quot;) +
  theme_minimal() +
  theme(legend.position = &quot;bottom&quot;) +
  scale_color_discrete(name = &quot;Product Name&quot;))</code>
  <boxed-text>
    <preformat>Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
ℹ Please use `linewidth` instead.</preformat>
  </boxed-text>
  <code language="r script"># Modify the visibility of traces
for (i in seq_along(p_ly$x$data)) {
  if (i == 1) {
    p_ly$x$data[[i]]$visible &lt;- TRUE  # Make the first trace visible
  } else {
    p_ly$x$data[[i]]$visible &lt;- &quot;legendonly&quot;  # Hide the rest
  }
}

# Plot
p_ly</code>
  <fig>
    <caption><p>Figure X Sale quantity of the most popular
    products</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/Top_Products_Quantity-1.png" />
  </fig>
  </sec>
  <p>Our proposed workaround is to aggregate Product_Name by
  Sub_Category, and treat it as a single product for the rest of the
  assignment, which we plotted in figure X.</p>
  <sec specific-use="notebook-content">
  <code language="r script"># Bar plots

# # Count frequency of top 20 products
# top_products &lt;- data %&gt;%
#   count(Product_Name, sort = TRUE) %&gt;%
#   top_n(20, n) %&gt;%
#   mutate(ProdName8 = substr(Product_Name, 1, 8))
# 
# # Plot!
# ggplot(top_products, aes(x = reorder(`ProdName8`, -n), y = n)) +
#   geom_bar(stat = &quot;identity&quot;, fill = &quot;steelblue&quot;) +
#   labs(title = &quot;Top 20 Most Sold Products&quot;,
#        x = &quot;Product Name&quot;,
#        y = &quot;Quantity sold&quot;) +
#   theme_minimal() +
#   coord_flip()
# 
# Count frequency of top 20 products
top_categories &lt;- data %&gt;%
  count(Sub_Category, sort = TRUE)
# 
# # Plot!
# ggplot(top_categories, aes(x = reorder(Sub_Category, -n), y = n)) +
#   geom_bar(stat = &quot;identity&quot;, fill = &quot;steelblue&quot;) +
#   labs(title = &quot;Sub_Categories sorted&quot;,
#        x = &quot;Product Name&quot;,
#        y = &quot;Quantity sold&quot;) +
#   theme_minimal() +
#   coord_flip()

# Find top 10 most sold product names
top_10_categories &lt;- top_categories$Sub_Category[0:10]

# Filter the data for  top 10 products
top_10_data &lt;- data %&gt;% filter(Sub_Category %in% top_10_categories)

# calculate sales per month
top_10_data &lt;- top_10_data %&gt;%
  mutate(Month = floor_date(Order_Date, unit = &quot;month&quot;))

# Aggregate data by month for each sub-category
top_10_data_aggregated &lt;- top_10_data %&gt;%
  group_by(Month, Sub_Category) %&gt;%
  summarise(Sales_Count = n(), .groups = 'drop')

# Some special interactive plot formatting (see previous plot)
p_ly &lt;- ggplotly(ggplot(top_10_data_aggregated, aes(x = Month, y = Sales_Count, color = Sub_Category, group = Sub_Category)) +
    geom_line(size = 1) +
    geom_point(size = 2) +
    labs(title = &quot;Monthly Sales for the Top 3 Most Sold Sub Categories&quot;,
         x = &quot;Month&quot;,
         y = &quot;Sales Count&quot;,
         color = &quot;Sub Category&quot;) +
    theme_minimal())

# Modify the visibility of traces
for (i in seq_along(p_ly$x$data)) {
  if (i == 1) {
    p_ly$x$data[[i]]$visible &lt;- TRUE  # Make the first trace visible
  } else {
    p_ly$x$data[[i]]$visible &lt;- &quot;legendonly&quot;  # Hide the rest
  }
}

# Plot
p_ly</code>
  <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/Aggregated_Sub_Category_sales-1.png" />
  </sec>
  <p>This aggregated Quantity starts to show trends and seasonality, and
  is much more useful to base predictions on! We will use these
  aggregated sub-categories for the rest of the assignment.</p>
  <p>To properly finish our data preprocessing we ran some statistics on
  Quantity aggregated by Sub_Category. Table x contains some descriptive
  statistics.</p>
  <sec specific-use="notebook-content">
  <code language="r script">library(dplyr)
library(kableExtra)

# Summarize the data
outlier_summary &lt;- data %&gt;%
  group_by(Sub_Category) %&gt;%
  summarize(
    Min = round(min(Quantity), 2),
    Mean = round(mean(Quantity), 2),
    Max = round(max(Quantity), 2),
    Sd = round(sd(Quantity), 2),
    CI_lower = round(Mean - 1.96 * (Sd / sqrt(n())), 2),
    CI_upper = round(Mean + 1.96 * (Sd / sqrt(n())), 2),
    .groups = &quot;drop&quot;
  )

# Output tables
kable(
  outlier_summary,
  caption = &quot;Statistics for Sub_Category quantity&quot;,
  format = &quot;pipe&quot;)</code>
  <table-wrap>
    <caption>
      <p>Statistics for Sub_Category quantity</p>
    </caption>
    <table>
      <thead>
        <tr>
          <th align="left">Sub_Category</th>
          <th align="right">Min</th>
          <th align="right">Mean</th>
          <th align="right">Max</th>
          <th align="right">Sd</th>
          <th align="right">CI_lower</th>
          <th align="right">CI_upper</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left">Accessories</td>
          <td align="right">1</td>
          <td align="right">3.84</td>
          <td align="right">14</td>
          <td align="right">2.28</td>
          <td align="right">3.68</td>
          <td align="right">4.00</td>
        </tr>
        <tr>
          <td align="left">Appliances</td>
          <td align="right">1</td>
          <td align="right">3.71</td>
          <td align="right">14</td>
          <td align="right">2.12</td>
          <td align="right">3.52</td>
          <td align="right">3.90</td>
        </tr>
        <tr>
          <td align="left">Art</td>
          <td align="right">1</td>
          <td align="right">3.77</td>
          <td align="right">14</td>
          <td align="right">2.13</td>
          <td align="right">3.62</td>
          <td align="right">3.92</td>
        </tr>
        <tr>
          <td align="left">Binders</td>
          <td align="right">1</td>
          <td align="right">3.92</td>
          <td align="right">14</td>
          <td align="right">2.29</td>
          <td align="right">3.80</td>
          <td align="right">4.04</td>
        </tr>
        <tr>
          <td align="left">Bookcases</td>
          <td align="right">1</td>
          <td align="right">3.81</td>
          <td align="right">13</td>
          <td align="right">2.28</td>
          <td align="right">3.51</td>
          <td align="right">4.11</td>
        </tr>
        <tr>
          <td align="left">Chairs</td>
          <td align="right">1</td>
          <td align="right">3.82</td>
          <td align="right">14</td>
          <td align="right">2.28</td>
          <td align="right">3.64</td>
          <td align="right">4.00</td>
        </tr>
        <tr>
          <td align="left">Copiers</td>
          <td align="right">1</td>
          <td align="right">3.44</td>
          <td align="right">9</td>
          <td align="right">1.83</td>
          <td align="right">3.01</td>
          <td align="right">3.87</td>
        </tr>
        <tr>
          <td align="left">Envelopes</td>
          <td align="right">1</td>
          <td align="right">3.57</td>
          <td align="right">9</td>
          <td align="right">2.05</td>
          <td align="right">3.32</td>
          <td align="right">3.82</td>
        </tr>
        <tr>
          <td align="left">Fasteners</td>
          <td align="right">1</td>
          <td align="right">4.21</td>
          <td align="right">14</td>
          <td align="right">2.41</td>
          <td align="right">3.89</td>
          <td align="right">4.53</td>
        </tr>
        <tr>
          <td align="left">Furnishings</td>
          <td align="right">1</td>
          <td align="right">3.72</td>
          <td align="right">14</td>
          <td align="right">2.16</td>
          <td align="right">3.58</td>
          <td align="right">3.86</td>
        </tr>
        <tr>
          <td align="left">Labels</td>
          <td align="right">1</td>
          <td align="right">3.85</td>
          <td align="right">14</td>
          <td align="right">2.35</td>
          <td align="right">3.61</td>
          <td align="right">4.09</td>
        </tr>
        <tr>
          <td align="left">Machines</td>
          <td align="right">1</td>
          <td align="right">3.83</td>
          <td align="right">11</td>
          <td align="right">2.17</td>
          <td align="right">3.43</td>
          <td align="right">4.23</td>
        </tr>
        <tr>
          <td align="left">Paper</td>
          <td align="right">1</td>
          <td align="right">3.78</td>
          <td align="right">14</td>
          <td align="right">2.23</td>
          <td align="right">3.66</td>
          <td align="right">3.90</td>
        </tr>
        <tr>
          <td align="left">Phones</td>
          <td align="right">1</td>
          <td align="right">3.70</td>
          <td align="right">14</td>
          <td align="right">2.19</td>
          <td align="right">3.56</td>
          <td align="right">3.84</td>
        </tr>
        <tr>
          <td align="left">Storage</td>
          <td align="right">1</td>
          <td align="right">3.73</td>
          <td align="right">14</td>
          <td align="right">2.19</td>
          <td align="right">3.58</td>
          <td align="right">3.88</td>
        </tr>
        <tr>
          <td align="left">Supplies</td>
          <td align="right">1</td>
          <td align="right">3.41</td>
          <td align="right">10</td>
          <td align="right">1.84</td>
          <td align="right">3.15</td>
          <td align="right">3.67</td>
        </tr>
        <tr>
          <td align="left">Tables</td>
          <td align="right">1</td>
          <td align="right">3.89</td>
          <td align="right">13</td>
          <td align="right">2.45</td>
          <td align="right">3.62</td>
          <td align="right">4.16</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  </sec>
  <p>The statistics for Quantity aggregated by Sub_Category looks valid.
  We can visualize it as histogram and check for anomalies. Figure y
  contains histograms of Quantity per Sub_Category.</p>
  <sec specific-use="notebook-content">
  <code language="r script">sub_categories &lt;- unique(data$Sub_Category)

p &lt;- plot_ly()
for (i in seq_along(sub_categories)) {
  sub &lt;- sub_categories[i]
  subset_data &lt;- data %&gt;% filter(Sub_Category == sub)
  p &lt;- add_trace(
    p,
    x = subset_data$Quantity,
    type = &quot;histogram&quot;,
    name = sub,
    visible = ifelse(i == 1, TRUE, FALSE)
  )
}

# We add a drop down menu for Sub_Category as toggling visibility in default ggplot2 adds the histograms up. Instead we want to be able to show each histogram seperately. 
dropdown_buttons &lt;- lapply(seq_along(sub_categories), function(i) {
  list(
    method = &quot;update&quot;,
    args = list(
      list(visible = lapply(seq_along(sub_categories), function(j) j == i)),
      list(xaxis = list(title = &quot;Quantity&quot;, autorange = TRUE), 
           yaxis = list(title = &quot;Frequency&quot;, autorange = TRUE))
    ),
    label = sub_categories[i]
  )
})

# Style drop down layout
p &lt;- p %&gt;%
  layout(
    title = &quot;Distribution of Quantity Sold per Order by Sub Category&quot;,
    xaxis = list(title = &quot;Quantity&quot;),
    yaxis = list(title = &quot;Frequency&quot;),
    showlegend = FALSE,  # Drop down instead of legend
    updatemenus = list(
      list(
        type = &quot;dropdown&quot;,
        buttons = dropdown_buttons,
        direction = &quot;down&quot;,
        x = 0.99,
        y = 0.99,
        showactive = TRUE,
        xanchor = &quot;left&quot;,
        yanchor = &quot;top&quot;
      )
    )
  )
p</code>
  <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/sub_category_histograms-1.png" />
  </sec>
  <p>The histograms show that the quantities a right-skewed distributed.
  This is to be expected since most orders contain only a small number
  of items. We will not remove the outliers with large quantities since
  they appear valid..</p>
</sec>
<sec id="data-visualization-nb-article">
  <title>3 Data Visualization</title>
</sec>
<sec id="forecasting-method-evaluation-nb-article">
  <title>4 Forecasting Method Evaluation</title>
  <sec id="forecasting-top-3-product-categories-4a-nb-article">
    <title>Forecasting top 3 product categories (4a)</title>
    <p>Let’s forecast sold quantities for the three most sold
    sub-categories:</p>
    <p>The steps taken for data preparation were:</p>
    <list list-type="bullet">
      <list-item>
        <p>Identifying Top Subcategories: The top three subcategories
        are selected from our dataset based on their sold quantities.
        The top three were: Binders, furnishing and paper.</p>
      </list-item>
      <list-item>
        <p>The sold quantities are aggregated monthly to create a time
        series object which we can use in the forecasting.</p>
      </list-item>
      <list-item>
        <p>A KPSS showed that the data is non stationary. First-order
        differencing is applied to transform the data from
        non-stationary to stationary. The KPSS results in a p-value
        &gt;0.05 showing the stationarity.</p>
      </list-item>
    </list>
    <sec specific-use="notebook-content">
    <code language="r script"># Find top 3 most sold product names
top_categories &lt;- data %&gt;%
  group_by(Sub_Category) %&gt;%
  summarise(Total_Quantity = sum(Quantity)) %&gt;%
  arrange(desc(Total_Quantity))
top_3_subcategories &lt;- top_categories$Sub_Category[0:3]

# Filter the data for  top 3 products
top_3_data &lt;- data %&gt;% filter(Sub_Category %in% top_3_subcategories)

# calculate sales per month
top_3_data &lt;- top_3_data %&gt;%
  mutate(Month = floor_date(Order_Date, unit = &quot;month&quot;))

# Aggregate data by month for each product
top_3_data_aggregated &lt;- top_3_data %&gt;%
  group_by(Month, Sub_Category) %&gt;%
  summarise(Sales_Count = n(), .groups = 'drop')

# Create a time series object for each product
ts_data &lt;- top_3_data_aggregated %&gt;%
  pivot_wider(names_from = Sub_Category, values_from = Sales_Count, values_fill = 0) %&gt;%
  select(-Month) %&gt;%
  as.matrix()

# Create a time series object
ts_data &lt;- ts(ts_data, start = c(2014, 1), end = c(2017, 12), frequency = 12)

# Create a time series list for each subcategory
ts_list &lt;- list()

for (subcategory in top_3_subcategories) {
  # Filter data for the subcategory
  subcategory_data &lt;- top_3_data_aggregated %&gt;% filter(Sub_Category == subcategory)

  # Create a time series object (assuming monthly data from January 2014 to December 2017)
  ts_list[[subcategory]] &lt;- ts(subcategory_data$Sales_Count,
                                start = c(2014, 1),
                                end = c(2017, 12),
                                frequency = 12)
}

#### 4 A
# Step 4: Apply forecasting methods to the top 3 sub-categories
forecast_results &lt;- list()  # Store results

for (subcategory in names(ts_list)) {
  ts_current &lt;- ts_list[[subcategory]]

  # Split the data into training and validation sets (70% training, 30% testing)
  train_size &lt;- floor(0.7 * length(ts_current))
  train_ts &lt;- window(ts_current, end = c(2014 + (train_size - 1) %/% 12, (train_size - 1) %% 12 + 1))
  test_ts &lt;- window(ts_current, start = c(2014 + train_size %/% 12, train_size %% 12 + 1))

  # 1. ARIMA
  arima_model &lt;- auto.arima(train_ts)
  arima_forecast &lt;- forecast(arima_model, h = length(test_ts))
  arima_accuracy &lt;- accuracy(arima_forecast, test_ts)

  # 2. Holt-Winters
  hw_model &lt;- HoltWinters(train_ts)
  hw_forecast &lt;- forecast(hw_model, h = length(test_ts))
  hw_accuracy &lt;- accuracy(hw_forecast, test_ts)

  # 3. ETS
  ets_model &lt;- ets(train_ts)
  ets_forecast &lt;- forecast(ets_model, h = length(test_ts))
  ets_accuracy &lt;- accuracy(ets_forecast, test_ts)

  # Store results
  forecast_results[[subcategory]] &lt;- list(
    ARIMA = list(Model = arima_model, Forecast = arima_forecast, Accuracy = arima_accuracy),
    HoltWinters = list(Model = hw_model, Forecast = hw_forecast, Accuracy = hw_accuracy),
    ETS = list(Model = ets_model, Forecast = ets_forecast, Accuracy = ets_accuracy)
  )
}


# For formatting, we ommitted almost all output. You can uncomment the code and check your output if you like.

# # Step 5: Print results
#  for (subcategory in names(forecast_results)) {
#   cat(&quot;\n\nResults for Sub_Category:&quot;, subcategory, &quot;\n&quot;)
# 
#   cat(&quot;\nARIMA Accuracy:\n&quot;)
#   print(forecast_results[[subcategory]]$ARIMA$Accuracy)
# 
#   cat(&quot;\nHolt-Winters Accuracy:\n&quot;)
#   print(forecast_results[[subcategory]]$HoltWinters$Accuracy)
# 
#   cat(&quot;\nETS Accuracy:\n&quot;)
#   print(forecast_results[[subcategory]]$ETS$Accuracy)
# }</code>
    </sec>
    <p>Three models are applied to each subcategory to forecast it. The
    models we use are: ARIMA, Holt-Winters and ETS. We have chosen these
    models because of their level of suitability for discrete time
    series data with all different levels of trend and seasonality. To
    evaluate the methods and its effectiveness , the data is split into
    a training set (70%) and testing set (30%).</p>
    <p>To assess the results, we use the following performance metrics:
    ME, RMSE, MAE and MAPE. They are calculated for the training and
    testing phases of the forecast.</p>
    <p>As we can see on the forecasting results ARIMA performed well for
    binders. We can state this because of the lowest RMSE if you compare
    it to the other models.</p>
    <sec specific-use="notebook-content">
    <code language="r script"># Lets plot the results
for (subcategory in names(forecast_results)) {
  ts_current &lt;- ts_list[[subcategory]]
  train_size &lt;- floor(0.7 * length(ts_current))
  test_ts &lt;- window(ts_current, start = c(2014 + train_size %/% 12, train_size %% 12 + 1))
  arima_forecast &lt;- forecast_results[[subcategory]]$ARIMA$Forecast
  hw_forecast &lt;- forecast_results[[subcategory]]$HoltWinters$Forecast
  ets_forecast &lt;- forecast_results[[subcategory]]$ETS$Forecast
  
  # Combined plot
  plot(arima_forecast$mean, col = &quot;blue&quot;, lwd = 2, 
       ylim = range(c(arima_forecast$mean, hw_forecast$mean, ets_forecast$mean, test_ts)),
       main = paste(&quot;Combined Forecasts for&quot;, subcategory, &quot;before differencing&quot;),
       xlab = &quot;Time&quot;, ylab = &quot;Forecasted Values&quot;)
  lines(test_ts, col = &quot;red&quot;, lty = 2, lwd = 2)
  lines(hw_forecast$mean, col = &quot;green&quot;, lwd = 2)
  lines(ets_forecast$mean, col = &quot;purple&quot;, lwd = 2)
  legend(&quot;topleft&quot;, legend = c(&quot;ARIMA&quot;, &quot;Holt-Winters&quot;, &quot;ETS&quot;, &quot;Test Data&quot;),
         col = c(&quot;blue&quot;, &quot;green&quot;, &quot;purple&quot;, &quot;red&quot;), lty = c(1, 1, 1, 2), lwd = 2)
}</code>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/Results1-1.png" />
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/Results1-2.png" />
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/Results1-3.png" />
    </sec>
    <sec specific-use="notebook-content">
    <code language="r script">options(warn = -1)
# # Step 6: Visualization of Forecasts
# for (subcategory in names(forecast_results)) {
#   plot(forecast_results[[subcategory]]$ARIMA$Forecast, main = paste(&quot;ARIMA Forecast for&quot;, subcategory))
#   lines(test_ts, col = &quot;red&quot;, lty = 2)
# 
#   plot(forecast_results[[subcategory]]$HoltWinters$Forecast, main = paste(&quot;Holt-Winters Forecast for&quot;, subcategory))
#   lines(test_ts, col = &quot;red&quot;, lty = 2)
# 
#   plot(forecast_results[[subcategory]]$ETS$Forecast, main = paste(&quot;ETS Forecast for&quot;, subcategory))
#   lines(test_ts, col = &quot;red&quot;, lty = 2)
# }

#more stationary tests
# Perform KPSS Test for the top 3 subcategories
# top_3_subcategories &lt;- top_categories$Sub_Category[0:3]
# 
# for (subcategory in top_3_subcategories) {
#   if (subcategory %in% names(ts_list)) {
#     ts_current &lt;- ts_list[[subcategory]]
#     cat(&quot;\nKPSS Test for Sub-Category:&quot;, subcategory, &quot;\n&quot;)
#     print(kpss.test(ts_current))
#   } else {
#     cat(&quot;\nSub-Category not found in ts_list:&quot;, subcategory, &quot;\n&quot;)
#   }
# }

#Because the all the 3 subcategory are non stationary because of a P value which is &lt;=0.05 we need to use differencing
# Apply differencing to each of the top 3 subcategories
differenced_series &lt;- list()

for (subcategory in top_3_subcategories) {
  if (subcategory %in% names(ts_list)) {
    ts_current &lt;- ts_list[[subcategory]]  # Get the time series for the subcategory
    ts_diff &lt;- diff(ts_current, differences = 1)  # Apply first-order differencing
    differenced_series[[subcategory]] &lt;- ts_diff  # Store the differenced series

    # Recheck stationarity with KPSS test
    #cat(&quot;\nKPSS Test for Differenced Sub-Category:&quot;, subcategory, &quot;\n&quot;)
    print(kpss.test(ts_diff))
  } else {
    #cat(&quot;\nSub-Category not found in ts_list:&quot;, subcategory, &quot;\n&quot;)
  }
}</code>
    <boxed-text>
      <preformat>
    KPSS Test for Level Stationarity

data:  ts_diff
KPSS Level = 0.10182, Truncation lag parameter = 3, p-value = 0.1


    KPSS Test for Level Stationarity

data:  ts_diff
KPSS Level = 0.061982, Truncation lag parameter = 3, p-value = 0.1


    KPSS Test for Level Stationarity

data:  ts_diff
KPSS Level = 0.098438, Truncation lag parameter = 3, p-value = 0.1</preformat>
    </boxed-text>
    </sec>
    <sec specific-use="notebook-content">
    <code language="r script"># Time differenced plots
# # Now P value is larger then 0.05 so we have stationary data
# # Plot the differenced series for each subcategory
# for (subcategory in top_3_subcategories) {
#   if (subcategory %in% names(differenced_series)) {
#     ts_diff &lt;- differenced_series[[subcategory]]
#     cat(&quot;\nPlotting Differenced Series for Sub-Category:&quot;, subcategory, &quot;\n&quot;)
#     plot(ts_diff, main = paste(&quot;Differenced Series for Sub-Category:&quot;, subcategory),
#          ylab = &quot;Differenced Values&quot;, xlab = &quot;Time&quot;)
#   }
# }


# Combine differenced series plots for all top subcategories
combined_diff_plot &lt;- function(differenced_series, top_3_subcategories) {
  plot(NULL, xlim = range(time(differenced_series[[top_3_subcategories[1]]])), 
       ylim = range(sapply(differenced_series[top_3_subcategories], range, na.rm = TRUE)),
       xlab = &quot;Time&quot;, ylab = &quot;Differenced Values&quot;,
       main = &quot;Differenced Series for Top Subcategories&quot;)
  colors &lt;- c(&quot;blue&quot;, &quot;green&quot;, &quot;purple&quot;)
  for (i in seq_along(top_3_subcategories)) {
    subcategory &lt;- top_3_subcategories[i]
    if (subcategory %in% names(differenced_series)) {
      ts_diff &lt;- differenced_series[[subcategory]]
      lines(ts_diff, col = colors[i], lwd = 2, lty = i)
    }
  }
  legend(&quot;topright&quot;, legend = top_3_subcategories, 
         col = colors, lty = 1:length(top_3_subcategories), lwd = 2)
}
combined_diff_plot(differenced_series, top_3_subcategories)</code>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/results3-1.png" />
    </sec>
    <sec specific-use="notebook-content">
    <code language="r script">#NEW FORECASTING FOR 4A with stationary data
# Step 4: Apply forecasting methods to the differenced top 3 sub-categories
forecast_results &lt;- list()  # Store results

for (subcategory in names(differenced_series)) {
  ts_current &lt;- differenced_series[[subcategory]]  # Use the differenced series

  # Split the data into training and validation sets (70% training, 30% testing)
  train_size &lt;- floor(0.7 * length(ts_current))
  train_ts &lt;- window(ts_current, end = c(2014 + (train_size - 1) %/% 12, (train_size - 1) %% 12 + 1))
  test_ts &lt;- window(ts_current, start = c(2014 + train_size %/% 12, train_size %% 12 + 1))

  # 1. ARIMA
  arima_model &lt;- auto.arima(train_ts)
  arima_forecast &lt;- forecast(arima_model, h = length(test_ts))
  arima_accuracy &lt;- accuracy(arima_forecast, test_ts)

  # 2. Holt-Winters
  hw_model &lt;- HoltWinters(train_ts)
  hw_forecast &lt;- forecast(hw_model, h = length(test_ts))
  hw_accuracy &lt;- accuracy(hw_forecast, test_ts)

  # 3. ETS
  ets_model &lt;- ets(train_ts)
  ets_forecast &lt;- forecast(ets_model, h = length(test_ts))
  ets_accuracy &lt;- accuracy(ets_forecast, test_ts)

  # Store results
  forecast_results[[subcategory]] &lt;- list(
    ARIMA = list(Model = arima_model, Forecast = arima_forecast, Accuracy = arima_accuracy),
    HoltWinters = list(Model = hw_model, Forecast = hw_forecast, Accuracy = hw_accuracy),
    ETS = list(Model = ets_model, Forecast = ets_forecast, Accuracy = ets_accuracy)
  )
}

# # Step 5: Print results
# for (subcategory in names(forecast_results)) {
#   cat(&quot;\n\nResults for Sub_Category:&quot;, subcategory, &quot;\n&quot;)
# 
#   cat(&quot;\nARIMA Accuracy:\n&quot;)
#   print(forecast_results[[subcategory]]$ARIMA$Accuracy)
# 
#   cat(&quot;\nHolt-Winters Accuracy:\n&quot;)
#   print(forecast_results[[subcategory]]$HoltWinters$Accuracy)
# 
#   cat(&quot;\nETS Accuracy:\n&quot;)
#   print(forecast_results[[subcategory]]$ETS$Accuracy)
# }

# # Step 6: Visualization of Forecasts
# for (subcategory in names(forecast_results)) {
#   plot(forecast_results[[subcategory]]$ARIMA$Forecast,
#        main = paste(&quot;ARIMA Forecast for&quot;, subcategory),
#        ylab = &quot;Differenced Values&quot;, xlab = &quot;Time&quot;)
#   lines(test_ts, col = &quot;red&quot;, lty = 2)
# 
#   plot(forecast_results[[subcategory]]$HoltWinters$Forecast,
#        main = paste(&quot;Holt-Winters Forecast for&quot;, subcategory),
#        ylab = &quot;Differenced Values&quot;, xlab = &quot;Time&quot;)
#   lines(test_ts, col = &quot;red&quot;, lty = 2)
# 
#   plot(forecast_results[[subcategory]]$ETS$Forecast,
#        main = paste(&quot;ETS Forecast for&quot;, subcategory),
#        ylab = &quot;Differenced Values&quot;, xlab = &quot;Time&quot;)
#   lines(test_ts, col = &quot;red&quot;, lty = 2)
# }

# Lets plot the results
for (subcategory in names(forecast_results)) {
  ts_current &lt;- ts_list[[subcategory]]
  train_size &lt;- floor(0.7 * length(ts_current))
  test_ts &lt;- window(ts_current, start = c(2014 + train_size %/% 12, train_size %% 12 + 1))
  arima_forecast &lt;- forecast_results[[subcategory]]$ARIMA$Forecast
  hw_forecast &lt;- forecast_results[[subcategory]]$HoltWinters$Forecast
  ets_forecast &lt;- forecast_results[[subcategory]]$ETS$Forecast
  
  # Combined plot
  plot(arima_forecast$mean, col = &quot;blue&quot;, lwd = 2, 
       ylim = range(c(arima_forecast$mean, hw_forecast$mean, ets_forecast$mean, test_ts)),
       main = paste(&quot;Combined Forecasts for&quot;, subcategory, &quot;before differencing&quot;),
       xlab = &quot;Time&quot;, ylab = &quot;Forecasted Values&quot;)
  lines(test_ts, col = &quot;red&quot;, lty = 2, lwd = 2)
  lines(hw_forecast$mean, col = &quot;green&quot;, lwd = 2)
  lines(ets_forecast$mean, col = &quot;purple&quot;, lwd = 2)
  legend(&quot;topleft&quot;, legend = c(&quot;ARIMA&quot;, &quot;Holt-Winters&quot;, &quot;ETS&quot;, &quot;Test Data&quot;),
         col = c(&quot;blue&quot;, &quot;green&quot;, &quot;purple&quot;, &quot;red&quot;), lty = c(1, 1, 1, 2), lwd = 2)
}</code>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/Results4-1.png" />
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/Results4-2.png" />
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/Results4-3.png" />
    </sec>
    <sec specific-use="notebook-content">
    <code language="r script"># final KPSS test

# Perform KPSS Test for the differenced series in the top 3 subcategories
for (subcategory in top_3_subcategories) {
  if (subcategory %in% names(differenced_series)) {
    ts_current &lt;- differenced_series[[subcategory]]  # Get the differenced series
    cat(&quot;\nKPSS Test for Differenced Sub-Category:&quot;, subcategory, &quot;\n&quot;)
    print(kpss.test(ts_current))
  } else {
    cat(&quot;\nSub-Category not found in differenced_series:&quot;, subcategory, &quot;\n&quot;)
  }
}</code>
    <boxed-text>
      <preformat>
KPSS Test for Differenced Sub-Category: Binders 

    KPSS Test for Level Stationarity

data:  ts_current
KPSS Level = 0.10182, Truncation lag parameter = 3, p-value = 0.1


KPSS Test for Differenced Sub-Category: Paper 

    KPSS Test for Level Stationarity

data:  ts_current
KPSS Level = 0.061982, Truncation lag parameter = 3, p-value = 0.1


KPSS Test for Differenced Sub-Category: Furnishings 

    KPSS Test for Level Stationarity

data:  ts_current
KPSS Level = 0.098438, Truncation lag parameter = 3, p-value = 0.1</preformat>
    </boxed-text>
    <code language="r script"># now they are all 0.1</code>
    </sec>
    <p>As we can see on the forecasting results ARIMA performed well for
    binders. We can state this because of the lowest RMSE. - ARIMA
    Binders: - Forecasting results Binders:</p>
    <p>For the subcategory furnishings we can see that the ETS
    forecasting method is the most stable across the training and
    testing phase. - ETS furnishings - Forecasting results
    Furnishings:</p>
    <p>For the last subcategory and product paper the ETS model is again
    the most consistent, comparing the statistics for training and test
    set. The high variability in the test data leads to larger
    forecasting errors in all the 3 models.</p>
    <list list-type="bullet">
      <list-item>
        <p>ETS Furnishings:</p>
      </list-item>
      <list-item>
        <p>Forecasting results:</p>
      </list-item>
    </list>
    <p>Residual Diagnostics: - The checks show no real autocorrelation
    for ARIMA models. Which indicates a good fitting forecast.</p>
  </sec>
  <sec id="conclusion-4a-nb-article">
    <title>Conclusion (4a)</title>
    <p>The most effective model is not the same in all the
    subcategories. Each model was validated based on its ability to
    capture seasonality and trend. ARIMA performed better for Binders,
    while ETS performed better for Furnishings and Paper.</p>
  </sec>
  <sec id="clustering-4b-nb-article">
    <title>Clustering (4b)</title>
    <sec specific-use="notebook-content">
    <code language="r script"># 4B
# 4B: Group Products into Clusters and Apply Forecasting Techniques
# Step 1: Extract Time-Series Features for Clustering
time_series_features &lt;- data.frame()

for (subcategory in names(ts_list)) {
  ts_current &lt;- ts_list[[subcategory]]

  # Decompose the time series to extract features
  decomposition &lt;- decompose(ts_current)
  trend_strength &lt;- var(decomposition$trend, na.rm = TRUE) / var(ts_current, na.rm = TRUE)
  seasonal_strength &lt;- var(decomposition$seasonal, na.rm = TRUE) / var(ts_current, na.rm = TRUE)
  random_strength &lt;- var(decomposition$random, na.rm = TRUE) / var(ts_current, na.rm = TRUE)

  # Store extracted features
  time_series_features &lt;- rbind(time_series_features,
                                data.frame(SubCategory = subcategory,
                                           TrendStrength = trend_strength,
                                           SeasonalStrength = seasonal_strength,
                                           RandomStrength = random_strength))
}

# Step 2: Normalize the Features for Clustering
time_series_features_scaled &lt;- time_series_features %&gt;%
  select(-SubCategory) %&gt;%
  scale()

# verify rows
# nrow(time_series_features_scaled)

# Step 3: Perform K-Means Clustering
# Determine the number of clusters dynamically
k &lt;- min(3, nrow(time_series_features_scaled))  # Set k to the smaller of 3 or the number of rows
# Hierarchical Clustering
distance_matrix &lt;- dist(time_series_features_scaled)  # Calculate distance matrix
hc &lt;- hclust(distance_matrix)  # Perform hierarchical clustering
time_series_features$Cluster &lt;- cutree(hc, k = k)  # Cut tree into 'k' clusters
# Add cluster information to the original data
time_series_features$Cluster &lt;- cutree(hc, k = k)

# Step 4: Apply Forecasting Techniques to Each Cluster
forecast_results_by_cluster &lt;- list()

for (cluster_id in unique(time_series_features$Cluster)) {
  # Get subcategories in the current cluster
  subcategories_in_cluster &lt;- time_series_features$SubCategory[time_series_features$Cluster == cluster_id]

  # Initialize storage for cluster results
  cluster_forecast_results &lt;- list()

  for (subcategory in subcategories_in_cluster) {
    if (subcategory %in% names(ts_list)) {
      ts_current &lt;- ts_list[[subcategory]]  # Access the time series

      # Split the data into training and validation sets (70% training, 30% testing)
      train_size &lt;- floor(0.7 * length(ts_current))
      train_ts &lt;- window(ts_current, end = c(2014 + (train_size - 1) %/% 12, (train_size - 1) %% 12 + 1))
      test_ts &lt;- window(ts_current, start = c(2014 + train_size %/% 12, train_size %% 12 + 1))

      # 1. ARIMA
      arima_model &lt;- auto.arima(train_ts)
      arima_forecast &lt;- forecast(arima_model, h = length(test_ts))
      arima_accuracy &lt;- accuracy(arima_forecast, test_ts)

      # 2. Holt-Winters
      hw_model &lt;- HoltWinters(train_ts)
      hw_forecast &lt;- forecast(hw_model, h = length(test_ts))
      hw_accuracy &lt;- accuracy(hw_forecast, test_ts)

      # 3. ETS
      ets_model &lt;- ets(train_ts)
      ets_forecast &lt;- forecast(ets_model, h = length(test_ts))
      ets_accuracy &lt;- accuracy(ets_forecast, test_ts)

      # Store results for the subcategory
      cluster_forecast_results[[subcategory]] &lt;- list(
        ARIMA = list(Model = arima_model, Forecast = arima_forecast, Accuracy = arima_accuracy),
        HoltWinters = list(Model = hw_model, Forecast = hw_forecast, Accuracy = hw_accuracy),
        ETS = list(Model = ets_model, Forecast = ets_forecast, Accuracy = ets_accuracy)
      )
    } else {
      cat(&quot;\nSub-Category not found in ts_list:&quot;, subcategory, &quot;\n&quot;)
    }
  }

  # Store results for the cluster
  forecast_results_by_cluster[[paste0(&quot;Cluster_&quot;, cluster_id)]] &lt;- cluster_forecast_results
}

# Step 5: Print Forecasting Accuracy for Each Cluster
for (cluster_id in names(forecast_results_by_cluster)) {
  cat(&quot;\n\nResults for&quot;, cluster_id, &quot;\n&quot;)
  cluster_results &lt;- forecast_results_by_cluster[[cluster_id]]

  for (subcategory in names(cluster_results)) {
    cat(&quot;\nSub-Category:&quot;, subcategory, &quot;\n&quot;)

    cat(&quot;\nARIMA Accuracy:\n&quot;)
    print(cluster_results[[subcategory]]$ARIMA$Accuracy)

    cat(&quot;\nHolt-Winters Accuracy:\n&quot;)
    print(cluster_results[[subcategory]]$HoltWinters$Accuracy)

    cat(&quot;\nETS Accuracy:\n&quot;)
    print(cluster_results[[subcategory]]$ETS$Accuracy)
  }
}</code>
    <boxed-text>
      <preformat>

Results for Cluster_1 

Sub-Category: Binders 

ARIMA Accuracy:
                    ME      RMSE      MAE        MPE     MAPE      MASE
Training set 0.7706014  4.643476 2.982256  0.6865304 12.91204 0.4854835
Test set     5.9407398 10.783528 7.616473 10.3681817 17.32927 1.2398909
                   ACF1 Theil's U
Training set 0.04429472        NA
Test set     0.04929320 0.3573866

Holt-Winters Accuracy:
                    ME     RMSE      MAE        MPE     MAPE      MASE
Training set 0.8668058 5.215491 3.789990 -0.4040374 15.36545 0.6169751
Test set     2.2473496 8.712049 6.243226  0.1597635 16.02219 1.0163391
                     ACF1 Theil's U
Training set -0.389045966        NA
Test set     -0.001830843 0.2777843

ETS Accuracy:
                   ME      RMSE      MAE       MPE     MAPE      MASE
Training set 0.743354  4.712561 3.656409  1.358692 15.41708 0.5952293
Test set     7.439784 12.216161 8.825094 10.667088 20.93561 1.4366433
                   ACF1 Theil's U
Training set -0.2225537        NA
Test set      0.0608554 0.3767484


Results for Cluster_2 

Sub-Category: Paper 

ARIMA Accuracy:
                    ME      RMSE       MAE        MPE     MAPE      MASE
Training set  1.117384  6.309464  3.827714   1.702165 14.32157 0.5468163
Test set     -9.014128 12.230774 10.375521 -30.103886 31.89519 1.4822173
                     ACF1 Theil's U
Training set -0.007064333        NA
Test set      0.108516273 0.6984566

Holt-Winters Accuracy:
                     ME     RMSE       MAE        MPE     MAPE      MASE
Training set   1.509544  7.27986  4.854547   3.325281 16.41309 0.6935067
Test set     -12.036865 14.48061 13.347292 -39.791842 41.51609 1.9067560
                    ACF1 Theil's U
Training set -0.02710216        NA
Test set      0.10006713  0.845232

ETS Accuracy:
                     ME      RMSE      MAE         MPE     MAPE      MASE
Training set 0.60941117  6.204628 4.474990 -1.66197953 18.73240 0.6392842
Test set     0.08500424 11.304488 8.247017 -0.08462612 20.47598 1.1781453
                    ACF1 Theil's U
Training set 0.005205508        NA
Test set     0.341519997  0.582549


Results for Cluster_3 

Sub-Category: Furnishings 

ARIMA Accuracy:
                    ME     RMSE      MAE       MPE     MAPE      MASE
Training set 0.0050974 3.873555 2.567868 -6.624673 20.20958 0.5559302
Test set     3.9523810 7.782677 6.238095 10.720079 22.75340 1.3505155
                    ACF1 Theil's U
Training set -0.20160077        NA
Test set     -0.03570035 0.6102339

Holt-Winters Accuracy:
                    ME     RMSE      MAE      MPE     MAPE      MASE
Training set 0.9137655 4.164677 3.475419 7.490317 24.75583 0.7524103
Test set     3.6371987 7.200985 5.673333 9.724318 20.50134 1.2282474
                    ACF1 Theil's U
Training set -0.43317305        NA
Test set      0.01804785 0.5689788

ETS Accuracy:
                    ME     RMSE      MAE        MPE     MAPE     MASE
Training set 0.7370579 3.466690 2.851745  0.4301648 20.85220 0.617388
Test set     6.0973038 8.354163 6.690832 20.7276401 23.55317 1.448531
                   ACF1 Theil's U
Training set -0.2087083        NA
Test set      0.3729315 0.7455977</preformat>
    </boxed-text>
    <code language="r script">## Step 6: Visualize the Clusters
library(ggplot2)

ggplot(time_series_features, aes(x = TrendStrength, y = SeasonalStrength, color = as.factor(Cluster))) +
  geom_point(size = 3) +
  labs(title = &quot;Clusters of Subcategories Based on Time-Series Features&quot;,
       x = &quot;Trend Strength&quot;, y = &quot;Seasonal Strength&quot;, color = &quot;Cluster&quot;) +
  theme_minimal()</code>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/somelable-1.png" />
    <code language="r script">#
#check
#residual diagnostic
for (cluster_id in names(forecast_results_by_cluster)) {
  cluster_results &lt;- forecast_results_by_cluster[[cluster_id]]
  for (subcategory in names(cluster_results)) {
    cat(&quot;\nResidual Diagnostics for Sub-Category:&quot;, subcategory, &quot;\n&quot;)
    checkresiduals(cluster_results[[subcategory]]$ARIMA$Model)
  }
}</code>
    <boxed-text>
      <preformat>
Residual Diagnostics for Sub-Category: Binders </preformat>
    </boxed-text>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/somelable-2.png" />
    <boxed-text>
      <preformat>
    Ljung-Box test

data:  Residuals from ARIMA(0,1,2)(0,1,0)[12]
Q* = 2.6295, df = 5, p-value = 0.7569

Model df: 2.   Total lags used: 7


Residual Diagnostics for Sub-Category: Paper </preformat>
    </boxed-text>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/somelable-3.png" />
    <boxed-text>
      <preformat>
    Ljung-Box test

data:  Residuals from ARIMA(0,1,1)(0,1,0)[12]
Q* = 6.6676, df = 6, p-value = 0.3527

Model df: 1.   Total lags used: 7


Residual Diagnostics for Sub-Category: Furnishings </preformat>
    </boxed-text>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/somelable-4.png" />
    <boxed-text>
      <preformat>
    Ljung-Box test

data:  Residuals from ARIMA(0,0,0)(0,1,0)[12] with drift
Q* = 9.5952, df = 7, p-value = 0.2127

Model df: 0.   Total lags used: 7</preformat>
    </boxed-text>
    <code language="r script"># P-value is higher then 0.1 so we have stationary data, this is good
#cluster level metrics
cluster_metrics &lt;- data.frame()
for (cluster_id in names(forecast_results_by_cluster)) {
  cluster_results &lt;- forecast_results_by_cluster[[cluster_id]]
  cluster_rmse &lt;- sapply(cluster_results, function(x) x$ARIMA$Accuracy[&quot;Test set&quot;, &quot;RMSE&quot;])
  cluster_mape &lt;- sapply(cluster_results, function(x) x$ARIMA$Accuracy[&quot;Test set&quot;, &quot;MAPE&quot;])
  cluster_metrics &lt;- rbind(cluster_metrics, data.frame(Cluster = cluster_id, MeanRMSE = mean(cluster_rmse), MeanMAPE = mean(cluster_mape)))
}
print(cluster_metrics)</code>
    <boxed-text>
      <preformat>    Cluster  MeanRMSE MeanMAPE
1 Cluster_1 10.783528 17.32927
2 Cluster_2 12.230774 31.89519
3 Cluster_3  7.782677 22.75340</preformat>
    </boxed-text>
    </sec>
    <p>Cluster 1 (e.g., Binders): ARIMA outperformed other methods due
    to significant autocorrelation and trend components.</p>
    <p>Cluster 2 (e.g., Furnishings): ETS was the most accurate method,
    effectively balancing trend and seasonality.</p>
    <p>Cluster 3 (e.g., Paper): ETS also performed best, with ARIMA
    showing higher error rates due to variability in random
    components.</p>
    <p>Residual diagnostics were performed for all ARIMA models,
    confirming no significant autocorrelation (p &gt; 0.05).</p>
    <p>Cluster-Level Metrics based on mean RMSE and MAPE show: - Cluster
    1 had the lowest RMSE using ARIMA. - Cluster 2 and 3 were better
    modeled with ETS</p>
  </sec>
  <sec id="conclusion-4b-nb-article">
    <title>Conclusion (4b)</title>
    <p>Clustering allows for tailored forecasting strategies. We
    conclude that for the given data set ARIMA is more effective for
    clusters with strong trends, while ETS is preferable for clusters
    with mixed seasonal and trend characteristics. The approach aligns
    with lecture notes, emphasizing the importance of adapting models
    based on time series characteristics.</p>
  </sec>
</sec>
<sec id="forecasting-future-values-nb-article">
  <title>5 Forecasting future values</title>
  <sec id="forecasting-3-products-5a-nb-article">
    <title>Forecasting 3 products (5a)</title>
    <p>In this session, we focused on evaluating different forecasting
    models (ARIMA, Holt-Winters, and ETS) for multiple sub-categories by
    analyzing their accuracy metrics, such as RMSE, MAPE, and residual
    diagnostics. Based on the evaluation results, we selected the
    best-performing model for each sub-category. We then used these
    models to forecast the future outcomes for each sub-category,
    projecting the data for the next year. Note: we may need to
    interpret the outcomes and explain why we pick the certain model</p>
    <sec specific-use="notebook-content">
    <code language="r script">#Binders-&gt;choose ARIMA
binders_ts &lt;- ts_list[[&quot;Binders&quot;]]
arima_model &lt;- auto.arima(binders_ts)
summary(arima_model)</code>
    <boxed-text>
      <preformat>Series: binders_ts 
ARIMA(1,1,1)(0,1,0)[12] 

Coefficients:
          ar1      ma1
      -0.4781  -0.4819
s.e.   0.2324   0.2426

sigma^2 = 51.18:  log likelihood = -117.97
AIC=241.94   AICc=242.72   BIC=246.61

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE     MASE        ACF1
Training set 0.864453 5.931761 4.092168 -1.363101 15.06142 0.558023 -0.03746012</preformat>
    </boxed-text>
    <code language="r script">arima_forecast &lt;- forecast(arima_model, h = 12)
print(arima_forecast)</code>
    <boxed-text>
      <preformat>         Point Forecast    Lo 80     Hi 80     Lo 95     Hi 95
Jan 2018       32.48390 23.31571  41.65208 18.462370  46.50543
Feb 2018       23.77181 14.59632  32.94731  9.739103  37.80452
Mar 2018       45.85265 35.59989  56.10541 30.172412  61.53289
Apr 2018       46.20475 35.63662  56.77287 30.042185  62.36730
May 2018       44.08009 32.93968  55.22051 27.042295  61.11789
Jun 2018       44.61784 33.06359  56.17210 26.947139  62.28855
Jul 2018       40.36072 28.34867  52.37277 21.989869  58.73157
Aug 2018       44.48366 32.05796  56.90937 25.480189  63.48714
Sep 2018       73.42488 60.58630  86.26346 53.789962  93.05979
Oct 2018       51.45299 38.22024  64.68573 31.215253  71.69072
Nov 2018       72.43955 58.82134  86.05775 51.612293  93.26680
Dec 2018       89.44597 75.45417 103.43777 68.047363 110.84458</preformat>
    </boxed-text>
    <code language="r script">plot(arima_forecast, main = &quot;ARIMA Forecast for Binders (Next 12 Months)&quot;)</code>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-18-1.png" />
    <code language="r script">#Paper-&gt;choose ETS
paper_ts &lt;- ts_list[[&quot;Paper&quot;]]
ets_model &lt;- ets(paper_ts)
summary(ets_model)</code>
    <boxed-text>
      <preformat>ETS(M,N,A) 

Call:
ets(y = paper_ts)

  Smoothing parameters:
    alpha = 0.3075 
    gamma = 1e-04 

  Initial states:
    l = 22.5954 
    s = 17.4472 16.5763 -4.1253 15.2986 0.421 -5.102
           -0.6145 -0.0341 -7.985 -2.6766 -15.6576 -13.5481

  sigma:  0.2365

     AIC     AICc      BIC 
365.1517 380.1517 393.2197 

Training set error measures:
                   ME     RMSE      MAE     MPE     MAPE      MASE       ACF1
Training set 1.450303 6.386648 4.166875 1.75373 14.03399 0.5245018 0.03600045</preformat>
    </boxed-text>
    <code language="r script">ets_forecast &lt;- forecast(ets_model, h = 12)
print(ets_forecast)</code>
    <boxed-text>
      <preformat>         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95
Jan 2018       30.45588 21.22661 39.68516 16.34092 44.57085
Feb 2018       28.34651 19.27484 37.41819 14.47258 42.22044
Mar 2018       41.32776 28.18367 54.47185 21.22561 61.42990
Apr 2018       36.01933 23.73891 48.29976 17.23804 54.80062
May 2018       43.97017 29.09477 58.84557 21.22021 66.72013
Jun 2018       43.39031 28.07408 58.70653 19.96616 66.81445
Jul 2018       38.90220 24.12853 53.67588 16.30782 61.49659
Aug 2018       44.42410 27.84663 61.00158 19.07104 69.77717
Sep 2018       59.30522 38.44478 80.16566 27.40193 91.20850
Oct 2018       39.87917 22.81850 56.93984 13.78712 65.97122
Nov 2018       60.58102 38.27853 82.88352 26.47230 94.68975
Dec 2018       61.45110 38.17748 84.72473 25.85717 97.04504</preformat>
    </boxed-text>
    <code language="r script">plot(ets_forecast, main = &quot;ETS Forecast for Paper (Next 12 Months)&quot;)</code>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-18-2.png" />
    <code language="r script">#Furnishings-&gt;choose ETS
furnishings_ts &lt;- ts_list[[&quot;Furnishings&quot;]]
ets_model &lt;- ets(furnishings_ts)
summary(ets_model)</code>
    <boxed-text>
      <preformat>ETS(M,A,A) 

Call:
ets(y = furnishings_ts)

  Smoothing parameters:
    alpha = 0.0438 
    beta  = 0.0437 
    gamma = 2e-04 

  Initial states:
    l = 15.4275 
    b = -0.1137 
    s = 13.3158 15.6269 -2.2962 10.1503 -5.0017 -2.448
           -3.4406 -1.0728 -1.1262 -4.3688 -11.689 -7.6497

  sigma:  0.2527

     AIC     AICc      BIC 
338.8888 359.2888 370.6992 

Training set error measures:
                    ME     RMSE      MAE        MPE    MAPE      MASE
Training set 0.6402485 3.793384 2.884208 -0.8302416 16.2441 0.5352139
                   ACF1
Training set 0.04613441</preformat>
    </boxed-text>
    <code language="r script">ets_forecast &lt;- forecast(ets_model, h = 12)
print(ets_forecast)</code>
    <boxed-text>
      <preformat>         Point Forecast    Lo 80    Hi 80     Lo 95    Hi 95
Jan 2018       21.37433 14.45350 28.29515 10.789837 31.95882
Feb 2018       18.56644 12.52244 24.61044  9.322944 27.80994
Mar 2018       27.11574 18.26943 35.96205 13.586481 40.64500
Apr 2018       31.58782 21.22159 41.95404 15.734048 47.44158
May 2018       32.87189 21.95559 43.78819 16.176845 49.56693
Jun 2018       31.73384 20.95052 42.51717 15.242170 48.22551
Jul 2018       33.95710 22.18459 45.72960 15.952604 51.96159
Aug 2018       32.63192 20.83816 44.42569 14.594916 50.66893
Sep 2018       49.01546 31.92061 66.11032 22.871140 75.15979
Oct 2018       37.79884 23.38308 52.21460 15.751844 59.84584
Nov 2018       56.95159 36.44698 77.45621 25.592490 88.31070
Dec 2018       55.87232 34.96477 76.77986 23.896983 87.84765</preformat>
    </boxed-text>
    <code language="r script">plot(ets_forecast, main = &quot;ETS Forecast for Furnishings (Next 12 Months)&quot;)</code>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-18-3.png" />
    </sec>
  </sec>
  <sec id="applying-to-all-data-5b-nb-article">
    <title>Applying to all data (5b)</title>
    <p>In this session, we first grouped the sub-categories into
    clusters based on key time-series features, including trend
    strength, seasonal strength, and random strength, using hierarchical
    clustering. Once the clusters were formed, we applied and evaluated
    multiple forecasting models—ARIMA, Holt-Winters, and ETS—on each
    sub-category within its respective cluster, comparing their accuracy
    metrics such as RMSE and MAPE. Based on the evaluation results, we
    selected the best-performing model for each sub-category and used it
    to forecast the future outcomes within a year, leveraging the
    clustering to enhance the accuracy and relevance of our
    predictions.</p>
    <sec specific-use="notebook-content">
    <code language="r script">#Cluster_Binders-&gt;Holt-Winters
cluster_id &lt;- 1
subcategory &lt;- &quot;Binders&quot;
hw_model &lt;- forecast_results_by_cluster[[paste0(&quot;Cluster_&quot;, cluster_id)]][[subcategory]]$HoltWinters$Model
hw_forecast &lt;- forecast(hw_model, h = 12)
print(hw_forecast)
plot(hw_forecast, main = &quot;Holt-Winters Forecast for Binders (Next 12 Months)&quot;, xlab = &quot;Time&quot;, ylab = &quot;Forecasted Values&quot;)

#Cluster_paper-&gt;ETS
cluster_id &lt;- 2
subcategory &lt;- &quot;Paper&quot;
ets_model &lt;- forecast_results_by_cluster[[paste0(&quot;Cluster_&quot;, cluster_id)]][[subcategory]]$ETS$Model
ets_forecast &lt;- forecast(ets_model, h = 12)
print(ets_forecast)
plot(ets_forecast, main = &quot;ETS Forecast for Paper (Next 12 Months)&quot;, xlab = &quot;Time&quot;, ylab = &quot;Forecasted Values&quot;)

#Cluster_Furnishings-&gt;Holt-Winters
cluster_id &lt;- 3
subcategory &lt;- &quot;Furnishings&quot;
hw_model &lt;- forecast_results_by_cluster[[paste0(&quot;Cluster_&quot;, cluster_id)]][[subcategory]]$HoltWinters$Model
hw_forecast &lt;- forecast(hw_model, h = 12)
print(hw_forecast)
plot(hw_forecast, main = &quot;Holt-Winters Forecast for Furnishings (Next 12 Months)&quot;, xlab = &quot;Time&quot;, ylab = &quot;Forecasted Values&quot;)</code>
    </sec>
  </sec>
</sec>
<sec id="forecast-interpretation-nb-article">
  <title>6 Forecast interpretation</title>
  <p>Lorem Ipsum</p>
  <p></p>
  <sec specific-use="notebook-content">
  <code language="r script"># Check for missing values
missing_values &lt;- colSums(is.na(data))
print(missing_values)  # Print missing values for reference
# heat map
library(Amelia)
missmap(data, main = &quot;Missing Data Pattern&quot;)
#distribution of key variables
#plot Quantity
ggplot(data, aes(x = Quantity)) +
  geom_histogram(binwidth = 1, fill = &quot;steelblue&quot;) +
  labs(title = &quot;Distribution of Quantity&quot;, x = &quot;Quantity&quot;, y = &quot;Frequency&quot;) +
  theme_minimal()
#plot sales
ggplot(data, aes(x = Sales)) +
  geom_histogram(binwidth = 50, fill = &quot;tomato&quot;) +
  labs(title = &quot;Distribution of Sales&quot;, x = &quot;Sales&quot;, y = &quot;Frequency&quot;) +
  theme_minimal()
# plot profit
ggplot(data, aes(x = Profit)) +
  geom_histogram(binwidth = 10, fill = &quot;purple&quot;) +
  labs(title = &quot;Distribution of Profit&quot;, x = &quot;Profit&quot;, y = &quot;Frequency&quot;) +
  theme_minimal()
# time based trends
data$Order_Date &lt;- as.Date(data$Order_Date, format = &quot;%Y-%m-%d&quot;)  # Ensure date format
time_series &lt;- data %&gt;%
  group_by(Order_Date) %&gt;%
  summarize(total_sales = sum(Sales), total_profit = sum(Profit), total_quantity = sum(Quantity))

ggplot(time_series, aes(x = Order_Date)) +
  geom_line(aes(y = total_sales, color = &quot;Sales&quot;)) +
  geom_line(aes(y = total_profit, color = &quot;Profit&quot;)) +
  geom_line(aes(y = total_quantity, color = &quot;Quantity&quot;)) +
  labs(title = &quot;Sales, Profit, and Quantity Over Time&quot;, x = &quot;Date&quot;, y = &quot;Value&quot;) +
  theme_minimal() +
  scale_color_manual(name = &quot;Metrics&quot;, values = c(&quot;Sales&quot; = &quot;blue&quot;, &quot;Profit&quot; = &quot;green&quot;, &quot;Quantity&quot; = &quot;red&quot;))

#sales by category and sub category
category_sales &lt;- data %&gt;%
  group_by(Category, Sub_Category) %&gt;%
  summarize(total_sales = sum(Sales))

ggplot(category_sales, aes(x = reorder(Sub_Category, -total_sales), y = total_sales, fill = Category)) +
  geom_bar(stat = &quot;identity&quot;) +
  labs(title = &quot;Sales by Category and Sub-Category&quot;, x = &quot;Sub-Category&quot;, y = &quot;Total Sales&quot;) +
  theme_minimal() +
  coord_flip()

#Outliers detection
#Quantity
ggplot(data, aes(x = Category, y = Quantity)) +
  geom_boxplot() +
  labs(title = &quot;Boxplot of Quantity by Category&quot;, x = &quot;Category&quot;, y = &quot;Quantity&quot;)
#sales
ggplot(data, aes(x = Category, y = Sales)) +
  geom_boxplot() +
  labs(title = &quot;Boxplot of Sales by Category&quot;, x = &quot;Category&quot;, y = &quot;Sales&quot;)

#profit
ggplot(data, aes(x = Category, y = Profit)) +
  geom_boxplot() +
  labs(title = &quot;Boxplot of Profit by Category&quot;, x = &quot;Category&quot;, y = &quot;Profit&quot;)
#Geo visualization

us_map &lt;- map_data(&quot;state&quot;)
if(&quot;State&quot; %in% colnames(data)) {
  state_sales &lt;- data %&gt;%
    group_by(State) %&gt;%
    summarize(total_sales = sum(Sales))

  # Convert state names to lowercase to match map data
  state_sales$State &lt;- tolower(state_sales$State)

  # Merge state sales data with map data
  state_sales_map &lt;- merge(us_map, state_sales, by.x = &quot;region&quot;, by.y = &quot;State&quot;, all.x = TRUE)

  # Plot sales by state
  ggplot(state_sales_map, aes(long, lat, group = group, fill = total_sales)) +
    geom_polygon(color = &quot;white&quot;) +
    scale_fill_continuous(low = &quot;lightblue&quot;, high = &quot;darkblue&quot;, na.value = &quot;gray90&quot;) +
    labs(title = &quot;Sales by State&quot;, fill = &quot;Total Sales&quot;) +
    theme_void() +
    coord_fixed(1.3)
}

# correlation matrix
numerical_data &lt;- data %&gt;% select(where(is.numeric))

cor_matrix &lt;- cor(numerical_data, use = &quot;complete.obs&quot;)

# Convert the correlation matrix to a long format
cor_data &lt;- as.data.frame(as.table(cor_matrix))

# Plot the correlation matrix using ggplot2
ggplot(cor_data, aes(Var1, Var2, fill = Freq)) +
  geom_tile(color = &quot;white&quot;) +
  scale_fill_gradient2(low = &quot;blue&quot;, high = &quot;red&quot;, mid = &quot;white&quot;,
                       midpoint = 0, limit = c(-1, 1), space = &quot;Lab&quot;,
                       name=&quot;Correlation&quot;) +
  geom_text(aes(label = round(Freq, 2)), color = &quot;black&quot;, size = 4) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1,
                                   size = 12, hjust = 1)) +
  coord_fixed() +
  labs(title = &quot;Correlation Matrix of Key Variables&quot;, x = &quot;&quot;, y = &quot;&quot;)</code>
  </sec>
  <sec id="forecasting-nb-article">
    <title>Forecasting??</title>
    <sec specific-use="notebook-content">
    <code language="r script">#Aggregate sales per month
monthly_sales &lt;- data %&gt;%
  mutate(Month = floor_date(Order_Date, &quot;month&quot;)) %&gt;%
  group_by(Month) %&gt;%
  summarize(total_sales = sum(Sales))
#Convert to time series
sales_ts &lt;- ts(monthly_sales$total_sales, frequency = 12, start = c(year(min(monthly_sales$Month)), month(min(monthly_sales$Month))))
#Arima model
arima_model &lt;- auto.arima(sales_ts)
arima_forecast &lt;- forecast(arima_model, h = 12)
autoplot(arima_forecast) + labs(title = &quot;ARIMA Forecast for Monthly Sales&quot;)</code>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-21-1.png" />
    <code language="r script">#Holts winter model
hw_model &lt;- HoltWinters(sales_ts)
hw_forecast &lt;- forecast(hw_model, h = 12)
autoplot(hw_forecast) + labs(title = &quot;Holt-Winters Forecast for Monthly Sales&quot;)</code>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-21-2.png" />
    <code language="r script"># clustering for segmentation
library(cluster)
#data clustering
clustering_data &lt;- data %&gt;%
  select(Sales, Quantity, Discount, Profit) %&gt;%
  na.omit()
set.seed(123)
kmeans_model &lt;- kmeans(clustering_data, centers = 3)
data$Cluster &lt;- as.factor(kmeans_model$cluster)
# visualize clustering result
ggplot(data, aes(x = Sales, y = Profit, color = Cluster)) +
  geom_point(alpha = 0.6) +
  labs(title = &quot;K-Means Clustering of Sales and Profit&quot;, x = &quot;Sales&quot;, y = &quot;Profit&quot;) +
  theme_minimal()</code>
    <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/unnamed-chunk-21-3.png" />
    </sec>
  </sec>
</sec>
</body>



<back>
</back>


</sub-article>
<sub-article article-type="notebook" id="nb-3-nb-1">
<front-stub>
<title-group>
<article-title>Supply Chain Data Analytics</article-title>
</title-group>
</front-stub>

<body>
<sec id="cell-849b3526-cc7a-49ca-a916-949194fa166d-nb-1" specific-use="notebook-content">
<p>Analyzing and Forcasting Supermarket Sales</p>
<p>Stan Brouwer (Vrije Universiteit)
Liz Chan (Master TSCM)
Maaike Lamberst (Supply Chain Data analysis)
Niek Schroor (Group 10)
December 5, 2024</p>
</sec>
<sec id="cell-5bd31506-0b72-4bf6-9fa2-790dcf547f9e-nb-1" specific-use="notebook-content">
</sec>
<sec id="da879eaf-acfa-4518-9b60-82eaed332469-nb-1" specific-use="notebook-content">
<p>Introduction</p>
</sec>
<sec id="cell-4907d4ff-82b5-47d9-8daf-5c22039972ba-nb-1" specific-use="notebook-content">
</sec>
<sec id="d7d31d56-c042-4b60-bba3-cd7e90f2ebff-nb-1" specific-use="notebook-content">
<p>We analyze, forecast and interpret the
<ext-link ext-link-type="uri" xlink:href="https://public.tableau.com/app/sample-data/sample_-_superstore.xls">Superstore
sales</ext-link> provided by
<ext-link ext-link-type="uri" xlink:href="https://public.tableau.com/app/learn/sample-data">Tableau</ext-link>
using different statistical and machine learning methods.</p>
<p>We describe our work in the PDF version. However, we would like to
recommend reading our quarto manuscript <italic>here</italic> as it
contains the <bold>relevant</bold> R code in the Article Notebook.</p>
<sec id="data-pre-processing-nb-1">
  <title>1 Data Pre-processing</title>
  <p>The superstore data set we selected is of high quality. Thus we do
  the required data pre-processing, but included the hypothetical steps
  we would take were our data of lower quality to communicate our
  understanding of the data pre-processing process.</p>
  <p>We took the following pre-processing steps:</p>
  </sec>
  <sec id="cell-69337c99-6b39-4ce9-a6d4-cc534cdb0f4f-nb-1" specific-use="notebook-content">
  <code language="python"># Clear workspace
rm(list = ls())
# Function to load (and install if necessary) dependencies
install_and_load &lt;- function(packages) {
  install.packages(setdiff(packages, rownames(installed.packages())), dependencies = TRUE)
  invisible(lapply(packages, require, character.only = TRUE))
}
install_and_load(c(&quot;tidyverse&quot;, &quot;readxl&quot;, &quot;ggplot2&quot;, &quot;lubridate&quot;, &quot;stats&quot;, &quot;Amelia&quot;,&quot;forecast&quot;, &quot;tseries&quot;, &quot;plotly&quot;, &quot;stringr&quot;, &quot;knitr&quot;))</code>
  <boxed-text>
    <preformat>Loading required package: tidyverse── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ ggplot2   3.5.1     ✔ tibble    3.2.1
✔ lubridate 1.9.3     ✔ tidyr     1.3.1
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors
Loading required package: readxl

Loading required package: AmeliaWarning: package 'Amelia' was built under R version 4.3.3Loading required package: Rcpp
## 
## Amelia II: Multiple Imputation
## (Version 1.8.3, built: 2024-11-07)
## Copyright (C) 2005-2024 James Honaker, Gary King and Matthew Blackwell
## Refer to http://gking.harvard.edu/amelia/ for more information
## 
Loading required package: forecastWarning: package 'forecast' was built under R version 4.3.3Registered S3 method overwritten by 'quantmod':
  method            from
  as.zoo.data.frame zoo 
Loading required package: tseriesWarning: package 'tseries' was built under R version 4.3.3Loading required package: plotly

Attaching package: 'plotly'

The following object is masked from 'package:ggplot2':

    last_plot

The following object is masked from 'package:stats':

    filter

The following object is masked from 'package:graphics':

    layout

Loading required package: knitrWarning: package 'knitr' was built under R version 4.3.3</preformat>
  </boxed-text>
  </sec>
  <sec id="cell-08adf834-669a-4137-ac53-473ed9f3c46a-nb-1" specific-use="notebook-content">
  <list list-type="bullet">
    <list-item>
      <p>Improved column names by removing whitespaces</p>
    </list-item>
    <list-item>
      <p>Removed the Row_ID column as it can be inferred by it’s
      index</p>
    </list-item>
    <list-item>
      <p>Removed all columns with a single unique value, as storing
      these would be
      <ext-link ext-link-type="uri" xlink:href="https://few.vu.nl/~molenaar/courses/StatR/chapters/B-06-raw_data.html">redundant</ext-link></p>
    </list-item>
    <list-item>
      <p>Ensured machine-readable date formats in yyyy-mm-dd as these
      usually differ per locale.</p>
    </list-item>
    <list-item>
      <p>Ensured proper decimal separators</p>
    </list-item>
    <list-item>
      <p>calculated the number of missing values (both NA and empty
      string ““) per column.</p>
    </list-item>
  </list>
  </sec>
  <sec id="cell-148d8508-c7d2-4e80-929a-2b1b6cd11f65-nb-1" specific-use="notebook-content">
  <code language="python"># Load the data
suppressWarnings({data &lt;- read_excel(&quot;data/sample_-_superstore.xls&quot;)}) # The Postal code column is stored as 'text' but coerced to numeric, causing warnings which we suppress

# Improve column names (replace &quot; &quot;with &quot;_&quot;)
colnames(data) &lt;- str_replace_all(colnames(data), &quot; &quot;, &quot;_&quot;)

# Remove the 'Row_ID' column as it can be inferred by it's index
data &lt;- subset(data, select = -`Row_ID`)

# Remove all columns that have only one unique value, as storing these would be redundant
data &lt;- data[, sapply(data, function(col) length(unique(col)) &gt; 1)]

# Ensure a machine-readable date format as these are usually horrible in excel files
data$Order_Date &lt;- as.Date(data$Order_Date, format = &quot;%Y-%m-%d&quot;)
data$Ship_Date &lt;- as.Date(data$Ship_Date, format = &quot;%Y-%m-%d&quot;)

# The readxl package by default uses the correct decimal separator (as opposed to base R)

# Calculate the number of missing values per column.
# The sample dates are likely in Unix time, and when these are converted to R date objects they are stored as Date objects (which are represented by 'double' datatypes). Comparing these dates to characters (empty strings) results in NA values. Thus we only check date values for NA. As Date objects are stored as doubles within R (amount of days since 1970-01-01), we can't check numeric columns for &quot; &quot; either. We thus only check character columns for &quot; &quot;.
missing_values &lt;- sapply(data, function(col) {
  if (inherits(col, &quot;Date&quot;)) {
    sum(is.na(col))
  } else if (is.character(col)) {
    sum(is.na(col) | col == &quot;&quot;)
  } else {
    sum(is.na(col))
  }
})

if (sum(missing_values) == 0) {
  print(&quot;No missing values&quot;)
}</code>
  <boxed-text>
    <preformat>[1] &quot;No missing values&quot;</preformat>
  </boxed-text>
  </sec>
  <sec id="b81e65cc-f83b-436e-b1e3-c1e79af543ad-nb-1" specific-use="notebook-content">
  <p>We also ran some descriptive statistics to check unlikely or
  impossible values, outliers, means, etc.</p>
  </sec>
  <sec id="cell-412655fd-4fc4-4406-a922-2a189b0c861e-nb-1" specific-use="notebook-content">
  <code language="python"># Select only numerical columns
numerical_data &lt;- data %&gt;% select_if(is.numeric)

# Apply sapply to calculate descriptive statistics for each numeric column
descriptive_stats &lt;- sapply(numerical_data, function(col) {
  # Remove NA values
  col &lt;- na.omit(col)
  
  # Calculate basic statistics
  min_val &lt;- min(col)
  max_val &lt;- max(col)
  mean_val &lt;- mean(col)
  median_val &lt;- median(col)
  
  # 95% Confidence Interval (assuming normal distribution)
  se &lt;- sd(col) / sqrt(length(col))  # Standard error
  ci_lower &lt;- mean_val - 1.96 * se  # Lower bound of 95% CI
  ci_upper &lt;- mean_val + 1.96 * se  # Upper bound of 95% CI
  
  # Calculate outliers (using 1.5 * IQR rule)
  Q1 &lt;- quantile(col, 0.25)
  Q3 &lt;- quantile(col, 0.75)
  IQR &lt;- Q3 - Q1
  outliers &lt;- sum(col &lt; (Q1 - 1.5 * IQR) | col &gt; (Q3 + 1.5 * IQR))  # Number of outliers
  
  # Return the statistics as a named vector
  return(c(Min = min_val, 
           Mean = mean_val, 
           Max = max_val, 
           Median = median_val, 
           `95% CI Lower` = ci_lower, 
           `95% CI Upper` = ci_upper, 
           Outliers = outliers))
})
head(data,5)</code>
  <boxed-text>
    <preformat># A tibble: 5 × 19
  Order_ID     Order_Date Ship_Date  Ship_Mode Customer_ID Customer_Name Segment
  &lt;chr&gt;        &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;  
1 CA-2016-152… 2016-11-08 2016-11-11 Second C… CG-12520    Claire Gute   Consum…
2 CA-2016-152… 2016-11-08 2016-11-11 Second C… CG-12520    Claire Gute   Consum…
3 CA-2016-138… 2016-06-12 2016-06-16 Second C… DV-13045    Darrin Van H… Corpor…
4 US-2015-108… 2015-10-11 2015-10-18 Standard… SO-20335    Sean O'Donne… Consum…
5 US-2015-108… 2015-10-11 2015-10-18 Standard… SO-20335    Sean O'Donne… Consum…
# ℹ 12 more variables: City &lt;chr&gt;, State &lt;chr&gt;, Postal_Code &lt;dbl&gt;,
#   Region &lt;chr&gt;, Product_ID &lt;chr&gt;, Category &lt;chr&gt;, `Sub-Category` &lt;chr&gt;,
#   Product_Name &lt;chr&gt;, Sales &lt;dbl&gt;, Quantity &lt;dbl&gt;, Discount &lt;dbl&gt;,
#   Profit &lt;dbl&gt;</preformat>
  </boxed-text>
  </sec>
  <sec id="f1aa39cb-136f-4f96-a52b-52a1135a912c-nb-1" specific-use="notebook-content">
  <p>There is some more processing to do, such as removing outliers.
  However, by doing so we impose our own assumptions on the data
  (possibly the outliers are actual sales?). We will visualize and
  qualitatively evaluate the data first, and then decide what other
  processing steps to take.</p>
  </sec>
  <sec id="cell-25ab37f1-d368-4c61-b41e-481172c82c7f-nb-1" specific-use="notebook-content">
  <code language="python"># Display the first 5 rows of the data in a nice table with kable
kable(head(data,5), caption = &quot;First 5 Rows of the Data&quot;, format = &quot;pipe&quot;)</code>
  <boxed-text>
  </boxed-text>
  </sec>
  <sec id="ca73d308-b7d2-43a9-80ad-e80854581beb-nb-1" specific-use="notebook-content">
</sec>
<sec id="section-nb-1">
  <title>2 Section</title>
  <p>This is a simple placeholder for the manuscript’s main document
  (<xref alt="knuth84?" rid="ref-knuth84-nb-1" ref-type="bibr"><bold>knuth84?</bold></xref>).</p>
  </sec>
  <sec id="cell-9c2fcecf-6445-4cc8-9218-b666bcdca55b-nb-1" specific-use="notebook-content">
  <code language="python">1 + 1</code>
  <boxed-text>
    <preformat>[1] 2</preformat>
  </boxed-text>
  </sec>
  <sec id="cell-20fe86a8-1a18-436d-a6f8-4ad67175020a-nb-1" specific-use="notebook-content">
</sec>
<sec id="introduction-nb-1">
  <title>3 Introduction</title>
  </sec>
  <sec id="af4d3d8d-1d5f-40ea-b014-2334e82bd866-nb-1" specific-use="notebook-content">
  <code language="python">eruptions &lt;- c(1492, 1585, 1646, 1677, 1712, 1949, 1971, 2021)
n_eruptions &lt;- length(eruptions)</code>
  </sec>
  <sec id="cell-fig-timeline-nb-1" specific-use="notebook-content">
  <code language="python">par(mar = c(3, 1, 1, 1) + 0.1)
plot(eruptions, rep(0, n_eruptions), 
  pch = &quot;|&quot;, axes = FALSE)
axis(1)
box()</code>
  <boxed-text>
  </boxed-text>
  </sec>
  <sec id="cell-74d96b0e-38ba-40e4-8a00-2874e7deed9e-nb-1" specific-use="notebook-content">
  <code language="python">avg_years_between_eruptions &lt;- mean(diff(eruptions[-n_eruptions]))
avg_years_between_eruptions</code>
  <boxed-text>
    <preformat>[1] 79.83333</preformat>
  </boxed-text>
  </sec>
  <sec id="cell-230104d7-c37b-438c-93fd-5bab58bad4dd-nb-1" specific-use="notebook-content">
  <p>Based on data up to and including 1971, eruptions on La Palma
  happen every 79.8 years on average.</p>
  <p>Studies of the magma systems feeding the volcano, such as Marrero
  et
  al. (<xref alt="2019" rid="ref-marrero2019-nb-1" ref-type="bibr">2019</xref>),
  have proposed that there are two main magma reservoirs feeding the
  Cumbre Vieja volcano; one in the mantle (30-40km depth) which charges
  and in turn feeds a shallower crustal reservoir (10-20km depth).</p>
  <p>Eight eruptions have been recorded since the late 1400s
  (Figure 1).</p>
  <p>Data and methods are discussed in Section 4.</p>
  <p>Let <inline-formula><alternatives>
  <tex-math><![CDATA[x]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
  denote the number of eruptions in a year. Then,
  <inline-formula><alternatives>
  <tex-math><![CDATA[x]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
  can be modeled by a Poisson distribution</p>
  <p><styled-content id="eq-poisson-nb-1"><disp-formula><alternatives>
  <tex-math><![CDATA[
  p(x) = \frac{e^{-\lambda} \lambda^{x}}{x !}
   \qquad(1)]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>−</mml:mi><mml:mi>λ</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>λ</mml:mi><mml:mi>x</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi>!</mml:mi></mml:mrow></mml:mfrac><mml:mspace width="2.0em"></mml:mspace><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></styled-content></p>
  <p>where <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>
  is the rate of eruptions per year. Using Equation 1, the probability
  of an eruption in the next <inline-formula><alternatives>
  <tex-math><![CDATA[t]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula>
  years can be calculated.</p>
  <table-wrap>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Year</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Current</td>
          <td>2021</td>
        </tr>
        <tr>
          <td>Teneguía</td>
          <td>1971</td>
        </tr>
        <tr>
          <td>Nambroque</td>
          <td>1949</td>
        </tr>
        <tr>
          <td>El Charco</td>
          <td>1712</td>
        </tr>
        <tr>
          <td>Volcán San Antonio</td>
          <td>1677</td>
        </tr>
        <tr>
          <td>Volcán San Martin</td>
          <td>1646</td>
        </tr>
        <tr>
          <td>Tajuya near El Paso</td>
          <td>1585</td>
        </tr>
        <tr>
          <td>Montaña Quemada</td>
          <td>1492</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <p>Table 1: Recent historic eruptions on La Palma</p>
  <p>Table 1 summarises the eruptions recorded since the colonization of
  the islands by Europeans in the late 1400s.</p>
  <p></p>
  <p>Figure 2: Map of La Palma</p>
  <p>La Palma is one of the west most islands in the Volcanic
  Archipelago of the Canary Islands (Figure 2).</p>
</sec>
<sec id="data-methods-nb-1">
  <title>4 Data &amp; Methods</title>
</sec>
<sec id="conclusion-nb-1">
  <title>5 Conclusion</title>
</sec>
<sec id="references-nb-1">
  <title>References</title>
  <p>Marrero, José, Alicia García, Manuel Berrocoso, Ángeles Llinares,
  Antonio Rodríguez-Losada, and R. Ortiz. 2019. “Strategies for the
  Development of Volcanic Hazard Maps in Monogenetic Volcanic Fields:
  The Example of La Palma (Canary Islands).” <italic>Journal of Applied
  Volcanology</italic> 8 (July).
  <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/s13617-019-0085-5">https://doi.org/10.1186/s13617-019-0085-5</ext-link>.</p>
  </sec>
</sec>
</body>



<back>
</back>


</sub-article>

</article>